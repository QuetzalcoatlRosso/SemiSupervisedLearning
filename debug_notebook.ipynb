{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.corpus import words as nltk_english_words\n",
    "\n",
    "from lib_utils.preprocessing import TextPreProcessor\n",
    "from lib_utils.expectation_maximization import EM_SSL\n",
    "\n",
    "\n",
    "\n",
    "# Set tokens to remove for all text preprocessing\n",
    "remove_zero_vocab_docs = True\n",
    "english_vocab = set(nltk_english_words.words())\n",
    "english_vocab = None\n",
    "_tokens_to_remove = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train raw data shape: (11314,)\n",
      "min, max full train data label vals = 0, 19\n",
      "unlabeled train data shape: (10000,)\n",
      "num avail train indices complement to unlabeled: 1314\n"
     ]
    }
   ],
   "source": [
    "# Fix static preprocessed data\n",
    "# original article suggests 10k fixed unlabeled samples\n",
    "processor = TextPreProcessor(n_unlabeled_train_samples=10000,\n",
    "                                    tokens_to_remove=_tokens_to_remove,\n",
    "                                    remove_zero_vocab_docs=remove_zero_vocab_docs,\n",
    "                                    english_vocab=english_vocab)\n",
    "# Initialize raw\n",
    "processor.set_static_full_train_raw_data()\n",
    "processor.set_static_raw_unlabeled_data()\n",
    "processor.set_static_raw_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train samples to select: 20\n",
      "at set_sample_raw_train_data\n",
      "size: (20,)\n",
      "sample labeled trained sentence\n",
      " \n",
      "I've got 7 episodes left on *Beta* for Sale at US$8 each (neg.), or\n",
      "for Trade 1-for-1 for movie on Beta or a used CD; or, a package deal\n",
      "for $50 or whatever you care to propose in trade -- e.g., all for a\n",
      "set of good stereo headphones (e.g. Sony V6 or V7), an Apple IWII\n",
      "sheet feeder, a good used FM/Cassette stereo \"walkman\" or a hotel\n",
      "coupon(s) for free stays FOB New York City (guests coming!)).  The\n",
      "remaining collection is as follows:\n",
      "\n",
      "         8 - Charlie X\n",
      "        11 - Dagger of the Mind\n",
      "        12 - Miri\n",
      "        17 - Shore Leave\n",
      "        20 - The Alternative Factor\n",
      "        29 - Operation-Annihilate!\n",
      "        33 - Who Mourns for Adonais?\n",
      "\n",
      "Numbers indicate episode numbering on the tape boxes, for those who\n",
      "are keeping track of what episodes they're missing in that manner.\n",
      "\n",
      "RSVP for summaries, if necessary.\n",
      "\n",
      "The tapes are all in excellent condition in the original packaging.\n",
      "All have been played at least once, but most have been played ONLY\n",
      "once, and NONE have been played more than twice. Running time: ~50\n",
      "min. ea.  (Unedited, uncut store-bought originals unlike those in\n",
      "syndication; all have *incredible* Beta HiFi sound!)\n",
      "\n",
      "I also have the following SF and Horror movies on Beta as well; US$10\n",
      "(negotiable) or Trade (1-for-1 swap for movie on Beta or a used CD):\n",
      "\n",
      "        The Bride (Sting, Jennifer Beales)\n",
      "*       Buck Rogers Conquers the Universe (Buster Crabbe, Constance Moore)\n",
      "\n",
      "RSVP for my larger Beta movies/music trade list, or find it on Misc.forsale!\n",
      "\n",
      "gld\n",
      "\n",
      "PS: For those of you who may wonder, Beta is alive as a pro/hobbyist\n",
      "format ... there's life beyond the corner video store! (-;\n",
      "Warning in process_documents_text: received single doc as str, not array; converting to array\n",
      "processed sentence\n",
      " ['got episodes left beta sale us neg trade movie beta used cd package deal whatever care propose trade e g set good stereo headphones e g sony v v apple iwii sheet feeder good used fm cassette stereo walkman hotel coupon free stays fob new york city guests coming remaining collection follows charlie x dagger mind miri shore leave alternative factor operation annihilate mourns adonais numbers indicate episode numbering tape boxes keeping track episodes missing manner rsvp summaries necessary tapes excellent condition original packaging played least played none played twice running time min ea unedited uncut store bought originals unlike syndication incredible beta hifi sound also following sf horror movies beta well us negotiable trade swap movie beta used cd bride sting jennifer beales buck rogers conquers universe buster crabbe constance moore rsvp larger beta movies music trade list find misc forsale gld ps may wonder beta alive pro hobbyist format life beyond corner video store']\n"
     ]
    }
   ],
   "source": [
    "processor.set_sample_raw_train_data()\n",
    "sample_sent = processor.labeled_train_data_sample[0]\n",
    "print('sample labeled trained sentence\\n', sample_sent)\n",
    "processed_sent = processor.process_documents_text(sample_sent)\n",
    "print('processed sentence\\n', processed_sent)\n",
    "\n",
    "doc_array_sample = processor.labeled_train_data_sample[:2]\n",
    "processed_docs = processor.process_documents_text(doc_array_sample)  # List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample count data before zero doc removal shape (20, 1015)\n",
      "Removing zero vocab docs from labeled train sample.\n",
      "After zero count doc removal:\n",
      " Kept 20 samples from original 11314 train\n",
      "labeled_train_sample_count_data shape: (20, 1015)\n",
      "got data=unlabeled, shape= (10000,)\n",
      "unlabeled count data shape (10000, 1015)\n",
      "unlabeld train count_data shape: (10000, 1015)\n",
      "test count data shape (7532, 1015)\n"
     ]
    }
   ],
   "source": [
    "# doc-to-vect based on train sample's count vectorizer\n",
    "processor.set_labeled_train_sample_count_data()\n",
    "print('labeled_train_sample_count_data shape:', processor.labeled_train_sample_count_data.shape)\n",
    "processor.set_unlabeled_count_data()\n",
    "print('unlabeld train count_data shape:', processor.unlabeled_count_data.shape)\n",
    "processor.set_test_count_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015\n",
      "['able', 'abstract', 'acceptable', 'accident', 'action', 'active', 'actual', 'adam', 'address', 'administering']\n",
      "['world', 'worth', 'would', 'wrist', 'years', 'yearwood', 'yes', 'yo', 'york', 'zone']\n",
      "['al', 'cd', 'cm', 'dl', 'ea', 'ed', 'et', 'fm', 'go', 'hm', 'hp', 'kg', 'mr', 'nd', 'pc', 'pp', 'ps', 'rd', 'sf', 'st', 'th', 'us', 'wc', 'yo']\n"
     ]
    }
   ],
   "source": [
    "# CHECK TRAIN SET VOCAB\n",
    "print(len(processor.vocab))\n",
    "print(processor.vocab[:10])\n",
    "print(processor.vocab[-10:])\n",
    "short_vocab_text = [v for v in processor.vocab if len(v) <=2]\n",
    "print(short_vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.5\n",
      "604\n"
     ]
    }
   ],
   "source": [
    "# LABELED TRAIN COUNT DATA STATS\n",
    "processor.get_train_doc_lengths()\n",
    "print(processor.med_doc_len)\n",
    "print(processor.max_doc_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivbarrie\\PycharmProjects\\SSL\\SemiSupervisedLearning\\lib_utils\\preprocessing.py:314: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  scaled_word_count_data = (static_doc_len / reshaped_sums) * word_count_data\n",
      "C:\\Users\\ivbarrie\\PycharmProjects\\SSL\\SemiSupervisedLearning\\lib_utils\\preprocessing.py:314: RuntimeWarning: invalid value encountered in multiply\n",
      "  scaled_word_count_data = (static_doc_len / reshaped_sums) * word_count_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# scale count data to trains' unif doc len\n",
    "scaled_labeled_train_sample_data = processor.make_uniform_doc_lens(word_count_data=processor.labeled_train_sample_count_data,\n",
    "                                                                  strategy='max')\n",
    "scaled_unlabeled_data = processor.make_uniform_doc_lens(word_count_data=processor.unlabeled_count_data,\n",
    "                                                        strategy='max')\n",
    "scaled_test_data = processor.make_uniform_doc_lens(word_count_data=processor.test_count_data,\n",
    "                                                   strategy='max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1015)\n",
      "[604. 604. 604. 604. 604. 604. 604. 604. 604. 604. 604. 604. 604. 604.\n",
      " 604. 604. 604. 604. 604. 604.]\n",
      "(10000, 1015)\n",
      "[604. 604. 604. ... 604. 604. 604.]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_labeled_train_sample_data.shape)\n",
    "print(np.sum(scaled_labeled_train_sample_data, axis=processor.vocab_axis))\n",
    "print(scaled_unlabeled_data.shape)\n",
    "print(np.sum(scaled_unlabeled_data, axis=processor.vocab_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample has 12 unique labels\n",
      "Checking initail M step on only labeled train data...\n",
      "Congrats, initial M step assertions passed.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = EM_SSL(labeled_count_data=scaled_labeled_train_sample_data,\n",
    "               label_vals=processor.train_sample_label_vals,\n",
    "               unlabeled_count_data=scaled_unlabeled_data,\n",
    "               max_em_iters=2,\n",
    "               min_em_loss_delta=2e-4)\n",
    "\n",
    "# model.fit()\n",
    "model.initialize_EM()  # only runs M step (compute thetas) on labeled train samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE CLOSER LOOK AT FIRST THETAS ON LABELED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1015)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labeled_count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.unlabeled_this_class_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "0.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.ordered_labels_list:\n",
    "    print(model.n_labeled_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.class_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.label_set:\n",
    "    print(model.n_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009852216748768472\n",
      "(20, 1015)\n",
      "word count avg, theta_j, theta_jt_avg, theta_jt_max\n",
      "0.60, 0.05, 0.00, 0.02\n",
      "0.60, 0.05, 0.00, 0.03\n",
      "0.60, 0.05, 0.00, 0.04\n",
      "0.60, 0.05, 0.00, 0.02\n",
      "1.79, 0.10, 0.00, 0.04\n",
      "1.79, 0.10, 0.00, 0.02\n",
      "1.19, 0.07, 0.00, 0.01\n",
      "1.79, 0.10, 0.00, 0.01\n",
      "1.19, 0.07, 0.00, 0.03\n",
      "0.60, 0.05, 0.00, 0.03\n",
      "0.60, 0.05, 0.00, 0.04\n",
      "0.60, 0.05, 0.00, 0.09\n"
     ]
    }
   ],
   "source": [
    "print(1 / model.vocab_size)\n",
    "print(model.theta_jt_per_class.shape)\n",
    "\n",
    "print(\"word count avg, theta_j, theta_jt_avg, theta_jt_max\")\n",
    "for j in model.label_set:\n",
    "    print(\"%0.2f, %0.2f, %0.2f, %0.2f\" % \n",
    "          (model.labeled_word_counts_per_class[j].mean(), model.theta_j_per_class[j], \n",
    "           model.theta_jt_per_class[j].mean(), model.theta_jt_per_class[j].max()))\n",
    "# TODO: theta_jt near zero => log(theta_jt) -> nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_per_class[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-sammple inference: test\n",
    "pct_test_correct_preds = model.evaluate_on_data(count_data=scaled_test_data,\n",
    "                                            label_vals=processor.full_test_label_vals)\n",
    "print(pct_test_correct_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.preds.dtype)\n",
    "print(processor.full_test_label_vals.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.preds\n",
    "p =  processor.full_test_label_vals\n",
    "print(m.shape == p.shape)\n",
    "corr = m == p\n",
    "print(np.sum(corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
