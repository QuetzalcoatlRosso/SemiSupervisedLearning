{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.corpus import words as nltk_english_words\n",
    "\n",
    "from lib_utils.preprocessing import TextPreProcessor\n",
    "from lib_utils.expectation_maximization import EM_SSL\n",
    "\n",
    "\n",
    "\n",
    "# Set tokens to remove for all text preprocessing\n",
    "remove_zero_vocab_docs = True\n",
    "english_vocab = set(nltk_english_words.words())\n",
    "english_vocab = None\n",
    "_tokens_to_remove = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train raw data shape: (11314,)\n",
      "min, max full train data label vals = 0, 19\n",
      "unlabeled train data shape: (500,)\n",
      "num avail train indices complement to unlabeled: 10814\n"
     ]
    }
   ],
   "source": [
    "# Fix static preprocessed data\n",
    "# original article suggests 10k fixed unlabeled samples\n",
    "processor = TextPreProcessor(n_labeled_train_samples=200,\n",
    "    n_unlabeled_train_samples=500,\n",
    "                                    tokens_to_remove=_tokens_to_remove,\n",
    "                                    remove_zero_vocab_docs=remove_zero_vocab_docs,\n",
    "                                    english_vocab=english_vocab)\n",
    "# Initialize raw\n",
    "processor.set_static_full_train_raw_data()\n",
    "processor.set_static_raw_unlabeled_data()\n",
    "processor.set_static_raw_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train samples to select: 200\n",
      "at set_sample_raw_train_data\n",
      "size: (200,)\n",
      "sample labeled trained sentence\n",
      " Stolen from Pasadena between 4:30 and 6:30 pm on 4/15.\n",
      "\n",
      "Blue and white Honda CBR900RR california plate KG CBR.   Serial number\n",
      "JH2SC281XPM100187, engine number 2101240.\n",
      "\n",
      "No turn signals or mirrors, lights taped over for track riders session\n",
      "at Willow Springs tomorrow.  Guess I'll miss it.  :-(((\n",
      "\n",
      "Help me find my baby!!!\n",
      "Warning in process_documents_text: received single doc as str, not array; converting to array\n",
      "processed sentence\n",
      " ['stolen pasadena pm blue white honda cbr rr california plate kg cbr serial number jh sc xpm engine number turn signals mirrors lights taped track riders session willow springs tomorrow guess miss help find baby']\n"
     ]
    }
   ],
   "source": [
    "processor.set_sample_raw_train_data()\n",
    "sample_sent = processor.labeled_train_data_sample[0]\n",
    "print('sample labeled trained sentence\\n', sample_sent)\n",
    "processed_sent = processor.process_documents_text(sample_sent)\n",
    "print('processed sentence\\n', processed_sent)\n",
    "\n",
    "doc_array_sample = processor.labeled_train_data_sample[:2]\n",
    "processed_docs = processor.process_documents_text(doc_array_sample)  # List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample count data before zero doc removal shape (200, 5632)\n",
      "Removing zero vocab docs from labeled train sample.\n",
      "After zero count doc removal:\n",
      " Kept 194 samples from original 11314 train\n",
      "labeled_train_sample_count_data shape: (194, 5632)\n",
      "got data=unlabeled, shape= (500,)\n",
      "unlabeled count data shape (500, 5632)\n",
      "unlabeld train count_data shape: (500, 5632)\n",
      "test count data shape (7532, 5632)\n"
     ]
    }
   ],
   "source": [
    "# doc-to-vect based on train sample's count vectorizer\n",
    "processor.set_labeled_train_sample_count_data()\n",
    "print('labeled_train_sample_count_data shape:', processor.labeled_train_sample_count_data.shape)\n",
    "processor.set_unlabeled_count_data()\n",
    "print('unlabeld train count_data shape:', processor.unlabeled_count_data.shape)\n",
    "processor.set_test_count_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5632\n",
      "['ab', 'abandon', 'abandond', 'abbie', 'abbreviation', 'abd', 'abide', 'abilities', 'ability', 'able']\n",
      "['zeit', 'zero', 'zeus', 'zip', 'zipguns', 'zlin', 'zone', 'zones', 'zupcic', 'zx']\n",
      "['ab', 'ac', 'ad', 'ah', 'al', 'au', 'av', 'ba', 'bb', 'bd', 'bg', 'bi', 'bj', 'br', 'bw', 'ca', 'cd', 'cg', 'co', 'cs', 'ct', 'cy', 'db', 'dc', 'de', 'dl', 'dr', 'dx', 'ed', 'ee', 'em', 'er', 'es', 'et', 'ex', 'fd', 'fg', 'fi', 'fm', 'gb', 'gl', 'gm', 'go', 'gs', 'ha', 'hb', 'hd', 'hi', 'hl', 'hp', 'hr', 'ht', 'hv', 'hz', 'ie', 'ig', 'ii', 'ip', 'jd', 'jh', 'kb', 'kg', 'kw', 'la', 'lb', 'lc', 'ld', 'li', 'mb', 'md', 'mi', 'mm', 'mo', 'mr', 'mt', 'mx', 'nb', 'nd', 'nj', 'nl', 'nm', 'nr', 'ns', 'ob', 'oh', 'ok', 'om', 'os', 'ot', 'oz', 'pc', 'ph', 'pm', 'po', 'pp', 'ps', 'pt', 'rj', 'rm', 'rn', 'rr', 'rt', 'rw', 'sb', 'sc', 'se', 'sp', 'st', 'su', 'tg', 'th', 'tt', 'tv', 'tx', 'uk', 'us', 'uw', 'vs', 'wb', 'wd', 'wf', 'wp', 'wu', 'ww', 'xt', 'ya', 'ye', 'yr', 'zx']\n"
     ]
    }
   ],
   "source": [
    "# CHECK TRAIN SET VOCAB\n",
    "print(len(processor.vocab))\n",
    "print(processor.vocab[:10])\n",
    "print(processor.vocab[-10:])\n",
    "short_vocab_text = [v for v in processor.vocab if len(v) <=2]\n",
    "print(short_vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n",
      "1157\n"
     ]
    }
   ],
   "source": [
    "# LABELED TRAIN COUNT DATA STATS\n",
    "processor.get_train_doc_lengths()\n",
    "print(processor.med_doc_len)\n",
    "print(processor.max_doc_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scale count data to trains' unif doc len\n",
    "scaled_labeled_train_sample_data = processor.make_uniform_doc_lens(word_count_data=processor.labeled_train_sample_count_data,\n",
    "                                                                  strategy='max')\n",
    "scaled_unlabeled_data = processor.make_uniform_doc_lens(word_count_data=processor.unlabeled_count_data,\n",
    "                                                        strategy='max')\n",
    "scaled_test_data = processor.make_uniform_doc_lens(word_count_data=processor.test_count_data,\n",
    "                                                   strategy='max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 5632)\n",
      "[1124.86111111 1128.075      1153.66570605 1079.86666667 1108.79166667\n",
      " 1079.86666667 1138.33870968 1131.84782609 1101.9047619  1128.7804878\n",
      " 1135.16981132 1145.07216495 1084.6875     1028.44444444 1139.46969697\n",
      " 1106.69565217 1138.03278689 1146.93913043 1124.86111111 1146.08490566\n",
      " 1051.81818182 1112.5        1088.94117647 1146.18691589 1128.7804878\n",
      " 1149.63057325 1150.11309524 1147.59349593 1121.93939394 1137.38983051\n",
      " 1088.94117647 1133.3877551  1153.83879781 1108.79166667 1114.14814815\n",
      " 1155.39305556 1099.15       1051.81818182 1150.42613636 1144.82105263\n",
      " 1104.40909091 1106.69565217  964.16666667 1135.16981132 1150.77956989\n",
      " 1146.66964286 1149.72327044 1147.59349593 1120.84375    1139.2\n",
      " 1120.84375    1143.54651163 1136.70175439 1108.79166667 1139.2\n",
      " 1132.89583333 1115.67857143 1096.10526316 1140.93055556 1068.\n",
      " 1153.63662791  867.75       1139.2        1123.94285714 1151.06666667\n",
      " 1104.40909091 1135.16981132 1151.99134199 1132.38297872 1121.93939394\n",
      " 1134.75       1138.921875   1150.97395833 1041.3        1127.33333333\n",
      " 1152.51550388  867.75       1125.72972973 1101.9047619  1084.6875\n",
      " 1130.09302326 1135.57407407 1135.96363636 1132.89583333 1104.40909091\n",
      "  925.6        1153.4617737  1117.10344828 1130.70454545 1140.23188406\n",
      " 1139.46969697 1152.21900826 1092.72222222 1133.86       1146.85087719\n",
      " 1123.94285714 1140.47142857 1133.86       1146.76106195 1121.93939394\n",
      " 1130.09302326 1104.40909091 1153.21895425 1110.72       1135.16981132\n",
      " 1108.79166667 1144.82105263 1115.67857143 1129.45238095 1131.28888889\n",
      " 1104.40909091 1130.09302326 1144.82105263 1136.70175439 1150.27325581\n",
      " 1114.14814815 1084.6875     1152.372      1137.71666667 1138.33870968\n",
      " 1142.8902439  1152.13865546 1140.23188406 1119.67741935 1143.06024096\n",
      " 1012.375      1136.70175439 1150.87830688 1140.47142857 1156.00086356\n",
      " 1132.38297872  964.16666667 1099.15       1119.67741935 1099.15\n",
      " 1126.55263158 1068.         1144.55913978 1092.72222222 1099.15\n",
      " 1144.42391304 1151.85777778 1140.47142857 1131.28888889 1142.71604938\n",
      " 1147.51639344 1135.96363636 1106.69565217 1138.63492063 1153.71306818\n",
      " 1146.57657658 1136.33928571 1144.28571429 1143.22619048 1096.10526316\n",
      " 1134.31372549 1152.74632353 1084.6875     1130.09302326 1124.86111111\n",
      " 1028.44444444 1148.73571429 1128.075      1084.6875     1135.16981132\n",
      " 1145.07216495 1117.10344828 1135.16981132 1096.10526316 1138.03278689\n",
      " 1108.79166667 1148.36567164 1153.50453172 1150.27325581 1112.5\n",
      " 1149.28666667 1133.3877551  1143.85227273 1130.70454545 1153.81267218\n",
      " 1143.06024096 1133.86       1068.         1088.94117647 1125.72972973\n",
      " 1134.75       1134.31372549 1140.23188406 1143.38823529 1139.73134328\n",
      " 1084.6875     1137.38983051 1101.9047619  1124.86111111]\n",
      "(500, 5632)\n",
      "[1121.93939394 1120.84375    1104.40909091 1096.10526316 1134.75\n",
      " 1137.71666667 1028.44444444 1128.7804878  1104.40909091 1148.55474453\n",
      " 1138.921875   1112.5        1028.44444444 1110.72       1124.86111111\n",
      " 1141.57333333 1099.15       1142.71604938 1115.67857143 1133.3877551\n",
      " 1079.86666667 1150.67759563  771.33333333 1147.02586207 1084.6875\n",
      " 1139.46969697 1068.         1106.69565217 1119.67741935 1138.63492063\n",
      " 1060.58333333    0.         1115.67857143 1118.43333333 1079.86666667\n",
      " 1096.10526316 1088.94117647 1147.27731092 1084.6875     1120.84375\n",
      " 1088.94117647    0.         1143.85227273 1114.14814815 1120.84375\n",
      " 1051.81818182 1110.72       1060.58333333 1130.70454545 1051.81818182\n",
      " 1092.72222222 1120.84375    1012.375      1110.72       1136.33928571\n",
      "  771.33333333 1122.97058824  964.16666667 1127.33333333 1079.86666667\n",
      " 1117.10344828 1104.40909091 1140.23188406 1128.075       964.16666667\n",
      " 1115.67857143 1126.55263158 1106.69565217 1145.07216495 1146.85087719\n",
      " 1127.33333333 1112.5         867.75       1145.31313131 1136.70175439\n",
      " 1106.69565217 1150.23391813 1119.67741935 1114.14814815 1145.76699029\n",
      " 1144.42391304 1041.3        1127.33333333 1134.31372549 1060.58333333\n",
      " 1124.86111111 1153.0779661  1153.36163522 1041.3        1138.921875\n",
      " 1108.79166667 1132.89583333 1101.9047619  1122.97058824 1099.15\n",
      " 1130.09302326 1122.97058824 1101.9047619  1099.15       1012.375\n",
      " 1141.15068493 1099.15       1115.67857143 1127.33333333  578.5\n",
      " 1114.14814815 1084.6875     1127.33333333 1012.375      1131.84782609\n",
      " 1051.81818182 1138.921875    925.6        1139.98529412 1068.\n",
      " 1115.67857143  578.5        1133.3877551  1139.73134328  991.71428571\n",
      " 1114.14814815 1154.07828283 1079.86666667 1149.90184049 1092.72222222\n",
      " 1012.375      1079.86666667 1147.35833333 1141.15068493 1068.\n",
      " 1140.23188406 1150.5        1012.375      1012.375      1104.40909091\n",
      " 1088.94117647 1144.14444444 1147.02586207 1139.2        1084.6875\n",
      " 1117.10344828 1125.72972973 1133.86       1119.67741935 1129.45238095\n",
      "  925.6        1148.90909091 1121.93939394 1088.94117647 1126.55263158\n",
      " 1119.67741935 1101.9047619  1092.72222222 1151.03608247 1141.36486486\n",
      " 1135.57407407 1148.67625899 1148.1        1130.70454545 1060.58333333\n",
      " 1068.            0.         1137.38983051 1139.46969697 1129.45238095\n",
      " 1119.67741935 1147.88976378 1150.03012048 1101.9047619  1115.67857143\n",
      " 1115.67857143 1106.69565217 1141.97402597 1140.70422535 1084.6875\n",
      " 1153.7037037  1126.55263158 1149.02068966 1117.10344828 1148.30075188\n",
      " 1118.43333333  925.6        1152.13865546 1115.67857143 1127.33333333\n",
      " 1074.35714286 1140.93055556 1118.43333333 1121.93939394 1147.19491525\n",
      "    0.         1130.70454545 1104.40909091 1088.94117647 1092.72222222\n",
      " 1136.70175439 1136.70175439 1133.3877551  1074.35714286 1106.69565217\n",
      " 1153.41795666 1148.79432624 1143.54651163  964.16666667 1132.38297872\n",
      " 1140.23188406 1152.65037594 1088.94117647 1132.89583333 1152.66666667\n",
      " 1012.375      1136.33928571  964.16666667 1136.33928571 1096.10526316\n",
      " 1112.5        1146.48181818    0.         1121.93939394    0.\n",
      "  925.6        1133.3877551  1151.66820276 1137.05172414 1028.44444444\n",
      " 1104.40909091 1114.14814815 1120.84375    1041.3        1130.70454545\n",
      " 1099.15       1110.72       1142.16666667 1101.9047619  1122.97058824\n",
      " 1156.69608616 1028.44444444 1147.43801653    0.         1117.10344828\n",
      " 1123.94285714 1138.921875   1144.28571429 1028.44444444 1112.5\n",
      " 1127.33333333 1132.89583333 1141.77631579  771.33333333 1126.55263158\n",
      " 1146.93913043 1139.46969697 1154.1075     1117.10344828 1106.69565217\n",
      " 1074.35714286 1140.23188406 1141.57333333 1150.87830688 1068.\n",
      " 1104.40909091 1121.93939394 1079.86666667 1096.10526316 1128.7804878\n",
      " 1115.67857143 1079.86666667 1139.2        1114.14814815 1074.35714286\n",
      " 1106.69565217 1147.9609375  1120.84375    1128.075      1096.10526316\n",
      " 1088.94117647 1117.10344828 1143.22619048 1088.94117647 1106.69565217\n",
      " 1104.40909091 1140.70422535 1096.10526316 1101.9047619  1141.97402597\n",
      " 1088.94117647 1112.5        1151.18592965 1149.67721519 1139.2\n",
      " 1151.41062802 1124.86111111 1146.28703704 1092.72222222 1124.86111111\n",
      " 1137.38983051 1068.         1146.08490566 1133.3877551  1012.375\n",
      " 1135.57407407 1012.375      1012.375       964.16666667 1084.6875\n",
      " 1092.72222222  964.16666667 1119.67741935 1131.28888889 1088.94117647\n",
      " 1127.33333333 1115.67857143 1140.23188406 1118.43333333 1121.93939394\n",
      " 1124.86111111    0.         1152.65037594  771.33333333 1137.38983051\n",
      " 1125.72972973  867.75       1127.33333333 1153.79501385 1137.05172414\n",
      " 1134.75       1130.70454545 1146.76106195 1106.69565217 1145.875\n",
      " 1145.65686275 1130.70454545 1155.69265537 1133.86       1144.14444444\n",
      " 1120.84375    1028.44444444  991.71428571 1129.45238095 1135.57407407\n",
      " 1068.         1145.76699029 1126.55263158 1121.93939394 1154.77927063\n",
      " 1144.         1136.70175439 1088.94117647 1041.3        1096.10526316\n",
      " 1143.22619048 1115.67857143 1146.48181818 1028.44444444 1141.36486486\n",
      "  867.75       1028.44444444 1119.67741935 1133.86       1152.80797101\n",
      " 1079.86666667  991.71428571 1127.33333333 1028.44444444 1128.075\n",
      " 1106.69565217 1155.71014493 1114.14814815 1106.69565217 1122.97058824\n",
      " 1079.86666667 1079.86666667 1119.67741935 1074.35714286 1088.94117647\n",
      " 1088.94117647  867.75       1092.72222222 1136.33928571 1142.35443038\n",
      " 1149.07534247 1150.84574468 1121.93939394  991.71428571 1138.63492063\n",
      " 1115.67857143  925.6        1060.58333333 1139.73134328 1145.76699029\n",
      " 1147.9609375  1142.71604938 1068.         1128.075      1051.81818182\n",
      " 1124.86111111 1148.30075188 1120.84375       0.         1134.75\n",
      " 1092.72222222 1152.40873016 1127.33333333 1096.10526316 1114.14814815\n",
      " 1123.94285714 1117.10344828 1084.6875     1099.15       1139.73134328\n",
      " 1138.921875   1012.375      1149.1292517  1132.38297872 1133.3877551\n",
      " 1117.10344828 1137.05172414 1084.6875     1068.         1135.16981132\n",
      " 1120.84375    1132.38297872 1088.94117647 1144.14444444 1138.63492063\n",
      " 1101.9047619  1140.47142857 1131.28888889 1152.35341365 1138.921875\n",
      " 1096.10526316 1135.57407407 1123.94285714 1110.72       1122.97058824\n",
      "  867.75       1137.05172414 1143.54651163 1130.70454545 1128.7804878\n",
      " 1012.375      1108.79166667 1135.57407407 1099.15       1120.84375\n",
      " 1104.40909091 1120.84375     964.16666667 1146.85087719 1141.57333333\n",
      " 1060.58333333  578.5        1154.84543762 1138.63492063 1130.09302326\n",
      " 1132.89583333    0.         1131.28888889 1104.40909091 1112.5\n",
      " 1128.075      1119.67741935 1153.99480519 1041.3        1151.9030837\n",
      " 1117.10344828 1123.94285714 1012.375      1127.33333333  991.71428571\n",
      " 1152.05555556 1135.57407407 1099.15       1117.10344828 1138.63492063\n",
      " 1041.3        1144.28571429 1092.72222222 1088.94117647    0.\n",
      " 1128.7804878  1051.81818182 1138.921875   1156.29278729 1153.63662791\n",
      " 1074.35714286 1068.         1096.10526316 1119.67741935  771.33333333\n",
      " 1138.33870968 1041.3        1068.         1099.15       1119.67741935\n",
      "  867.75       1155.16349206  991.71428571 1142.71604938 1119.67741935]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_labeled_train_sample_data.shape)\n",
    "print(np.sum(scaled_labeled_train_sample_data, axis=processor.vocab_axis))\n",
    "print(scaled_unlabeled_data.shape)\n",
    "print(np.sum(scaled_unlabeled_data, axis=processor.vocab_axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize EM (only first M step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample has 20 unique labels\n",
      "Checking initail M step on only labeled train data...\n",
      "Congrats, initial M step assertions passed.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = EM_SSL(labeled_count_data=scaled_labeled_train_sample_data,\n",
    "               label_vals=processor.train_sample_label_vals,\n",
    "               unlabeled_count_data=scaled_unlabeled_data,\n",
    "               max_em_iters=2,\n",
    "               min_em_loss_delta=2e-4)\n",
    "\n",
    "model.initialize_EM()  # only runs M step (compute thetas) on labeled train samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabeled data class probas min: 0.00, avg: 0.00, max 0.00\n"
     ]
    }
   ],
   "source": [
    "print('unlabeled data class probas min: %0.2f, avg: %0.2f, max %0.2f' %\n",
    "     ( model.unlabeled_data_class_probas.min(), \n",
    "      model.unlabeled_data_class_probas.mean(), \n",
    "      model.unlabeled_data_class_probas.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classs in labeled train 20\n",
      "1.5841880856960797\n"
     ]
    }
   ],
   "source": [
    "print('num classs in labeled train', len(model.label_set))\n",
    "print(model.word_counts_per_class[0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(model.total_word_count_per_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently only labeled data: False\n",
      "true label of single doc: 19\n",
      "Denominator terms\n",
      "\n",
      "total word count for class  19:  6586.426147113544\n",
      "min expected proba: 8.184360145567832e-05\n",
      "log of min expected proba -9.410700431232465\n",
      "Numerator terms\n",
      "\n",
      "labeled max word counts for class 19: 267.0\n",
      "single doc word count min, avg: 0.0 0.20484121201205135\n",
      "theta_j_vocab min for class 19: 8.184360145567832e-05\n"
     ]
    }
   ],
   "source": [
    "# problem to solve, when computing: labeled loss log (P(yi = cj|θ) .P(xi|yi = cj; θ)) ,\n",
    "# P(xi|yi = cj; θ) -> 0 => log() -> nan\n",
    "# and the reason P(xi|yi = cj; θ) -> 0 is bc theta_j_vocab -> 0 => theta_j_vocab^w_t -> 0 for w_t > 0\n",
    "# Recall  P(xi|yi = cj; θ) = product_t \\theta_j_t^ w_t\n",
    "## So log(P(xi|yi = cj; θ) ) = sum_t (w_t * log(\\theta_j_t())\n",
    "### So just need theta_j_t > 0, likelihood improved with smaller vocab size or smaller unif. doc length\n",
    "\n",
    "# TODO: implement above log trick for theta_j_vocab\n",
    "\n",
    "print('currently only labeled data:', model.only_labeled_data)\n",
    "single_doc = model.labeled_count_data[2]\n",
    "this_true_label =  model.label_vals[2]\n",
    "print('true label of single doc:', this_true_label)\n",
    "# Denominators for single doc per class proba\n",
    "print('Denominator terms\\n')\n",
    "print('total word count for class % d: ' % this_true_label, model.total_word_count_per_class[this_true_label])\n",
    "# compute minimum proba: for words not appearing in class\n",
    "min_expected_proba = (1 / (model.vocab_size + model.total_word_count_per_class[this_true_label]))\n",
    "print('min expected proba:', min_expected_proba)\n",
    "print('log of min expected proba', np.log(min_expected_proba))\n",
    "print('Numerator terms\\n')\n",
    "# Numerators for single doc per class proba\n",
    "print('labeled max word counts for class %d:' % this_true_label, model.labeled_word_counts_per_class[this_true_label].max())\n",
    "print('single doc word count min, avg:', single_doc.min(), single_doc.mean())\n",
    "print('theta_j_vocab min for class %d:' % this_true_label, model.theta_j_vocab_per_class[this_true_label].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS ON SINGLE TRAIN LABEL DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check P(c_j | theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta j shape (20,)\n",
      "sum theta j == 1:  True\n"
     ]
    }
   ],
   "source": [
    "theta_j = model.theta_j_per_class  # per class: (n_docs_in_class + 1) / (self.n_docs + self.n_labels)\n",
    "print('theta j shape', theta_j.shape)\n",
    "print('sum theta j == 1: ', np.isclose(np.sum(theta_j, axis=0), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  FIX: P(x_i | c_j, theta): debug + logSumExp trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of log_prob(w_t | cj): (20, 5632)\n",
      "log_prob(w_t | cj) min, mean, max\n",
      "-9.97693863157124 -9.456397612265821 -3.7976641575335726\n",
      "doc unnorm LOG probas per class of single do avg, min: -10173.024557322897 -10848.653344136104\n",
      "TEST doc unnorm LOG probas per class of single do avg, min: -10173.024557322897 -10848.653344136104\n",
      "argmax unnorm LOG proba class 19\n",
      "TEST argmax unnorm LOG proba class 19\n"
     ]
    }
   ],
   "source": [
    "# FIX: P(x_i | c_j, theta) -> {theta}_{tj} =: theta_j_vocab -> non-nan computations\n",
    "X_log = np.log(model.theta_j_vocab_per_class)\n",
    "print('shape of log_prob(w_t | cj):', X_log.shape)\n",
    "print('log_prob(w_t | cj) min, mean, max')\n",
    "print(X_log.min(), X_log.mean(), X_log.max())\n",
    "\n",
    "# COMPUTE UNNORMALIZED LOG PROBAS\n",
    "doc_log_proba_per_class = np.log(model.theta_j_per_class) +  np.array([np.sum(single_doc * np.log(model.theta_j_vocab_per_class[j]), axis=0)\n",
    "                                            for j in model.ordered_labels_list])\n",
    "test_doc_log_proba_per_class = model.compute_unnormalized_class_log_probas_doc(single_doc)\n",
    "print('doc unnorm LOG probas per class of single do avg, min:', doc_log_proba_per_class.mean(), doc_log_proba_per_class.min())\n",
    "print('TEST doc unnorm LOG probas per class of single do avg, min:', test_doc_log_proba_per_class.mean(), test_doc_log_proba_per_class.min())\n",
    "print('argmax unnorm LOG proba class', np.argmax(doc_log_proba_per_class))\n",
    "print('TEST argmax unnorm LOG proba class', np.argmax(test_doc_log_proba_per_class))\n",
    "\n",
    "# fixed_doc_proba_per_class = np.exp(doc_log_proba_per_class)\n",
    "# print('FIXED doc probas per class of single do avg, min:', fixed_doc_proba_per_class.mean(), fixed_doc_proba_per_class.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start LogSumExp trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test doc log probas per class (20,)\n",
      "given unnormalized log probas prior to logSumExp [ -9841.49258555 -10543.12898415 -10465.86560467 -10848.65334414\n",
      " -10442.89939051 -10659.34738913 -10241.41139516 -10254.84416874\n",
      " -10346.79989939 -10537.52837456 -10373.32594466 -10002.10649368\n",
      " -10286.09976215 -10079.15406636 -10131.42183657 -10085.95208996\n",
      " -10013.85346456 -10045.00224081 -10040.62872427  -8220.97538746]\n",
      "summand [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "log of sums -8220.975387463148\n",
      "test log probas [-1620.51719809 -2322.15359668 -2244.8902172  -2627.67795667\n",
      " -2221.92400305 -2438.37200167 -2020.43600769 -2033.86878128\n",
      " -2125.82451192 -2316.5529871  -2152.3505572  -1781.13110622\n",
      " -2065.12437469 -1858.17867889 -1910.4464491  -1864.97670249\n",
      " -1792.87807709 -1824.02685334 -1819.6533368      0.        ]\n",
      "dtype test log probas float64\n",
      "test probas normalized [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "test probas norm idx 3 0.0\n",
      "argmax:  19\n"
     ]
    }
   ],
   "source": [
    "print('shape of test doc log probas per class', test_doc_log_proba_per_class.shape)\n",
    "#print('log sum', test_doc_log_proba_per_class.sum())\n",
    "#def compute_sum_of_logs_from_log_prbas(log_probas):\n",
    "#\"\"\"For single doc x, compute denom: log P(x|theta), without yet using P(c_j|theta)\"\"\"\n",
    "log_probas = np.copy(test_doc_log_proba_per_class)\n",
    "print('given unnormalized log probas prior to logSumExp', log_probas)\n",
    "# log sum exp trick: see Murphy\n",
    "max_log = np.max(log_probas)\n",
    "summand = [np.exp(a - max_log) for a in log_probas]  # still have underflow...\n",
    "print('summand', summand)\n",
    "log_of_sums = np.log(np.sum(summand)) + max_log  #+ min_expected_proba # log(denom)\n",
    "print('log of sums', log_of_sums)\n",
    "test_log_probas_normalized  = log_probas - log_of_sums\n",
    "print('test log probas', test_log_probas_normalized)\n",
    "print('dtype test log probas', test_log_probas_normalized.dtype)\n",
    "test_probas_normalized = np.exp(test_log_probas_normalized)\n",
    "print('test probas normalized', test_probas_normalized)\n",
    "print('test probas norm idx 3', test_probas_normalized[3])\n",
    "print('argmax: ', np.argmax(test_probas_normalized))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X = np.sum(np.exp(log_probas - max_log))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_log_probas [ -9841.49258555 -10543.12898415 -10465.86560467 -10848.65334414\n",
      " -10442.89939051 -10659.34738913 -10241.41139516 -10254.84416874\n",
      " -10346.79989939 -10537.52837456 -10373.32594466 -10002.10649368\n",
      " -10286.09976215 -10079.15406636 -10131.42183657 -10085.95208996\n",
      " -10013.85346456 -10045.00224081 -10040.62872427  -8220.97538746]\n"
     ]
    }
   ],
   "source": [
    "# formalize as methods\n",
    "\n",
    "def compute_unnormalized_class_log_probas(doc):\n",
    "    \"\"\"Get unnormalized log probas\"\"\"\n",
    "    u_log_probas =  np.log(model.theta_j_per_class) +  np.array([np.sum(doc * np.log(model.theta_j_vocab_per_class[j]), axis=0)\n",
    "                                            for j in model.ordered_labels_list])\n",
    "    return u_log_probas\n",
    "\n",
    "\n",
    "    \n",
    "def compute_doc_class_probas(u_log_probas):\n",
    "    \"\"\"Apply normalization compute normalization factor using log-sum-exp trick.\"\"\"\n",
    "    max_log = np.max(u_log_probas)\n",
    "    summand = np.sum(np.exp(log_probas - max_log))\n",
    "    log_of_sums = np.log(summand) + max_log  #+ min_expected_proba # log(denom)\n",
    "    log_probas_normalized  = log_probas - log_of_sums\n",
    "    class_probas_normalized = np.exp(log_probas_normalized)\n",
    "    return class_probas_normalized\n",
    "\n",
    "u_log_probas = compute_unnormalized_class_log_probas(single_doc)\n",
    "print('u_log_probas', u_log_probas)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE CLOSER LOOK AT FIRST THETAS ON LABELED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 5632)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labeled_count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.unlabeled_this_class_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "8.0\n",
      "12.0\n",
      "12.0\n",
      "10.0\n",
      "12.0\n",
      "10.0\n",
      "10.0\n",
      "14.0\n",
      "12.0\n",
      "13.0\n",
      "5.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "11.0\n",
      "14.0\n",
      "8.0\n",
      "6.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.ordered_labels_list:\n",
    "    print(model.n_labeled_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.class_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "8.0\n",
      "12.0\n",
      "12.0\n",
      "10.0\n",
      "12.0\n",
      "10.0\n",
      "10.0\n",
      "14.0\n",
      "12.0\n",
      "13.0\n",
      "5.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "11.0\n",
      "14.0\n",
      "8.0\n",
      "6.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.label_set:\n",
    "    print(model.n_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001775568181818182\n",
      "(20, 5632)\n",
      "word count avg, theta_j, theta_jt_avg, theta_jt_max\n",
      "1.58, 0.04, 0.00, 0.01\n",
      "1.59, 0.04, 0.00, 0.01\n",
      "2.40, 0.06, 0.00, 0.02\n",
      "2.36, 0.06, 0.00, 0.01\n",
      "1.99, 0.05, 0.00, 0.02\n",
      "2.38, 0.06, 0.00, 0.01\n",
      "1.99, 0.05, 0.00, 0.01\n",
      "2.01, 0.05, 0.00, 0.01\n",
      "2.82, 0.07, 0.00, 0.01\n",
      "2.30, 0.06, 0.00, 0.02\n",
      "2.60, 0.07, 0.00, 0.01\n",
      "0.99, 0.03, 0.00, 0.02\n",
      "1.40, 0.04, 0.00, 0.01\n",
      "1.56, 0.04, 0.00, 0.02\n",
      "1.61, 0.04, 0.00, 0.01\n",
      "2.22, 0.06, 0.00, 0.01\n",
      "2.79, 0.07, 0.00, 0.01\n",
      "1.62, 0.04, 0.00, 0.01\n",
      "1.22, 0.03, 0.00, 0.01\n",
      "1.17, 0.03, 0.00, 0.02\n"
     ]
    }
   ],
   "source": [
    "print(1 / model.vocab_size)\n",
    "print(model.theta_j_vocab_per_class.shape)\n",
    "\n",
    "print(\"word count avg, theta_j, theta_jt_avg, theta_jt_max\")\n",
    "for j in model.label_set:\n",
    "    print(\"%0.2f, %0.2f, %0.2f, %0.2f\" % \n",
    "          (model.labeled_word_counts_per_class[j].mean(), model.theta_j_per_class[j], \n",
    "           model.theta_j_vocab_per_class[j].mean(), model.theta_j_vocab_per_class[j].max()))\n",
    "# TODO: theta_jt near zero => log(theta_jt) -> nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03271028037383177"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_per_class[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.1694648698710128 0.0 267.0 6586.426147113544\n"
     ]
    }
   ],
   "source": [
    "print(model.word_counts_per_class[j].min(), model.word_counts_per_class[j].mean(), \n",
    "      model.word_counts_per_class[j].min(), model.word_counts_per_class[j].max(), \n",
    "    model.word_counts_per_class[j].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6586.426147113544"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.total_word_count_per_class[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5632,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_vocab_per_class[j].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001775568181818182, 0.02193408519012179)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_vocab_per_class[j].mean(), model.theta_j_vocab_per_class[j].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't want to compute np.log(model.theta_j_vocab_per_class) directly since theta_j_vocab is sparse => underflow\n",
    "#np.log(model.theta_j_vocab_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Train fit: sanity check\n",
    "pct_train_correct_preds = model.evaluate_on_data(count_data=scaled_labeled_train_sample_data,\n",
    "                                            label_vals=processor.train_sample_label_vals)\n",
    "print(pct_train_correct_preds)  # without a full EM loop, should be equal to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499\n",
      "0.33178438661710036\n"
     ]
    }
   ],
   "source": [
    "# out-of-sammple inference: test\n",
    "pct_test_correct_preds = model.evaluate_on_data(count_data=scaled_test_data,\n",
    "                                            label_vals=processor.full_test_label_vals)\n",
    "print(pct_test_correct_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(model.preds.dtype)\n",
    "print(processor.full_test_label_vals.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.33178438661710036\n"
     ]
    }
   ],
   "source": [
    "m = model.preds\n",
    "p =  processor.full_test_label_vals\n",
    "print(m.shape == p.shape)\n",
    "corr = m == p\n",
    "print(np.sum(corr) / len(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EM Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  E-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.E_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta j shape (20,)\n",
      "sum theta j == 1:  True\n"
     ]
    }
   ],
   "source": [
    "theta_j = model.theta_j_per_class  # per class: (n_docs_in_class + 1) / (self.n_docs + self.n_labels)\n",
    "print('theta j shape', theta_j.shape)\n",
    "print('sum theta j == 1: ', np.isclose(np.sum(theta_j, axis=0), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabeled data class probas min: 0.00, avg: 0.05, max 1.00\n"
     ]
    }
   ],
   "source": [
    "print('unlabeled data class probas min: %0.2f, avg: %0.2f, max %0.2f' %\n",
    "     ( model.unlabeled_data_class_probas.min(), \n",
    "      model.unlabeled_data_class_probas.mean(), \n",
    "      model.unlabeled_data_class_probas.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word count for class  19:  6586.426147113544\n"
     ]
    }
   ],
   "source": [
    "print('total word count for class % d: ' % this_true_label, model.total_word_count_per_class[this_true_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 20)\n"
     ]
    }
   ],
   "source": [
    "u_joint_factors = np.apply_along_axis(func1d=model.compute_unnormalized_class_log_probas_doc,\n",
    "                                            axis=model.vocab_axis,\n",
    "                                            arr=model.unlabeled_count_data)\n",
    "print(u_joint_factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "u_log_of_sums = np.apply_along_axis(func1d=model.compute_log_of_sums,\n",
    "                                    axis=1,\n",
    "                                    arr=u_joint_factors)\n",
    "print(u_log_of_sums.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4302732.242912833\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(u_log_of_sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.M_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word count for class  19:  18695.778969314593\n"
     ]
    }
   ],
   "source": [
    "print('total word count for class % d: ' % this_true_label, model.total_word_count_per_class[this_true_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss = 5930965.02\n"
     ]
    }
   ],
   "source": [
    "print('total loss = %0.2f' % model.compute_total_loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.E_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.20974294 -3.70210723 -2.49900142 -3.1478135  -2.53547739 -2.94604664\n",
      " -3.11878968 -2.97951963 -2.76022969 -2.99731838 -2.91453634 -3.33964579\n",
      " -3.2970942  -3.65581391 -3.10110156 -2.53432959 -2.55377146 -3.03140861\n",
      " -3.18741854 -3.66071884]\n"
     ]
    }
   ],
   "source": [
    "print(np.log(model.theta_j_per_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word count for class  19:  18695.780761045135\n",
      "-1428176.3915571177 -3362907.908171191 -4791084.299728309\n",
      "total loss = 5930169.61\n"
     ]
    }
   ],
   "source": [
    "model.M_step()\n",
    "print('total word count for class % d: ' % this_true_label, model.total_word_count_per_class[this_true_label])\n",
    "labeled_loss = model.compute_labeled_loss()\n",
    "unlabeled_loss = model.compute_unlabeled_loss()\n",
    "print(labeled_loss, unlabeled_loss, labeled_loss + unlabeled_loss)\n",
    "print('total loss = %0.2f' % model.compute_total_loss())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
