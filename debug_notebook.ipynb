{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.corpus import words as nltk_english_words\n",
    "\n",
    "from lib_utils.preprocessing import TextPreProcessor\n",
    "from lib_utils.expectation_maximization import EM_SSL\n",
    "\n",
    "\n",
    "\n",
    "# Set tokens to remove for all text preprocessing\n",
    "remove_zero_vocab_docs = True\n",
    "english_vocab = set(nltk_english_words.words())\n",
    "english_vocab = None\n",
    "_tokens_to_remove = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train raw data shape: (11314,)\n",
      "min, max full train data label vals = 0, 19\n",
      "unlabeled train data shape: (500,)\n",
      "num avail train indices complement to unlabeled: 10814\n"
     ]
    }
   ],
   "source": [
    "# Fix static preprocessed data\n",
    "# original article suggests 10k fixed unlabeled samples\n",
    "processor = TextPreProcessor(n_labeled_train_samples=200,\n",
    "    n_unlabeled_train_samples=500,\n",
    "                                    tokens_to_remove=_tokens_to_remove,\n",
    "                                    remove_zero_vocab_docs=remove_zero_vocab_docs,\n",
    "                                    english_vocab=english_vocab)\n",
    "# Initialize raw\n",
    "processor.set_static_full_train_raw_data()\n",
    "processor.set_static_raw_unlabeled_data()\n",
    "processor.set_static_raw_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train samples to select: 200\n",
      "at set_sample_raw_train_data\n",
      "size: (200,)\n",
      "sample labeled trained sentence\n",
      " \n",
      "\n",
      "\n",
      "I've even seen pictures of an installation where the ham pulled a little \n",
      "trailer behind his car with a 4KW generator, and ran the full legal limit \n",
      "while mobile. I don't know what his gas mileage was like, though, or \n",
      "where he found resonators able to stand the gaff.\n",
      "\n",
      "Warning in process_documents_text: received single doc as str, not array; converting to array\n",
      "processed sentence\n",
      " ['even seen pictures installation ham pulled little trailer behind car kw generator ran full legal limit mobile know gas mileage like though found resonators able stand gaff']\n"
     ]
    }
   ],
   "source": [
    "processor.set_sample_raw_train_data()\n",
    "sample_sent = processor.labeled_train_data_sample[0]\n",
    "print('sample labeled trained sentence\\n', sample_sent)\n",
    "processed_sent = processor.process_documents_text(sample_sent)\n",
    "print('processed sentence\\n', processed_sent)\n",
    "\n",
    "doc_array_sample = processor.labeled_train_data_sample[:2]\n",
    "processed_docs = processor.process_documents_text(doc_array_sample)  # List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample count data before zero doc removal shape (200, 5250)\n",
      "Removing zero vocab docs from labeled train sample.\n",
      "After zero count doc removal:\n",
      " Kept 198 samples from original 11314 train\n",
      "labeled_train_sample_count_data shape: (198, 5250)\n",
      "got data=unlabeled, shape= (500,)\n",
      "unlabeled count data shape (500, 5250)\n",
      "unlabeld train count_data shape: (500, 5250)\n",
      "test count data shape (7532, 5250)\n"
     ]
    }
   ],
   "source": [
    "# doc-to-vect based on train sample's count vectorizer\n",
    "processor.set_labeled_train_sample_count_data()\n",
    "print('labeled_train_sample_count_data shape:', processor.labeled_train_sample_count_data.shape)\n",
    "processor.set_unlabeled_count_data()\n",
    "print('unlabeld train count_data shape:', processor.unlabeled_count_data.shape)\n",
    "processor.set_test_count_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5250\n",
      "['abc', 'ability', 'able', 'abo', 'abort', 'abraham', 'absence', 'absolute', 'absolutely', 'absorb']\n",
      "['zaven', 'zenit', 'zezel', 'zing', 'zip', 'zone', 'zoo', 'zooid', 'zoom', 'zx']\n",
      "['ac', 'ae', 'ah', 'al', 'ao', 'ar', 'ba', 'bt', 'ca', 'cd', 'ce', 'cf', 'cg', 'cl', 'co', 'cs', 'cv', 'dc', 'de', 'dr', 'ds', 'du', 'dx', 'ea', 'ed', 'eg', 'eh', 'em', 'eq', 'er', 'et', 'fd', 'fg', 'fi', 'fm', 'gc', 'gd', 'gi', 'gm', 'go', 'gp', 'hd', 'hi', 'hp', 'hq', 'hr', 'ht', 'id', 'ie', 'ig', 'ii', 'io', 'iv', 'jn', 'jr', 'kb', 'kp', 'ku', 'kw', 'la', 'le', 'lh', 'li', 'lo', 'ls', 'mb', 'mc', 'md', 'mg', 'mi', 'mm', 'mn', 'mo', 'mr', 'ms', 'mt', 'mx', 'nd', 'ne', 'nj', 'nl', 'ns', 'nt', 'ny', 'oh', 'ok', 'op', 'os', 'ou', 'pa', 'pb', 'pc', 'pd', 'ph', 'pp', 'ps', 'pt', 'qc', 'rc', 'rd', 'rf', 'ri', 'rm', 'rs', 'rw', 'sa', 'se', 'sg', 'sj', 'sk', 'st', 'sx', 'tb', 'tc', 'td', 'th', 'tt', 'tv', 'un', 'us', 'va', 'vc', 'wb', 'xv', 'ya', 'ye', 'yr', 'zx']\n"
     ]
    }
   ],
   "source": [
    "# CHECK TRAIN SET VOCAB\n",
    "print(len(processor.vocab))\n",
    "print(processor.vocab[:10])\n",
    "print(processor.vocab[-10:])\n",
    "short_vocab_text = [v for v in processor.vocab if len(v) <=2]\n",
    "print(short_vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "# LABELED TRAIN COUNT DATA STATS\n",
    "processor.get_train_doc_lengths()\n",
    "print(processor.med_doc_len)\n",
    "print(processor.max_doc_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scale count data to trains' unif doc len\n",
    "scaled_labeled_train_sample_data = processor.make_uniform_doc_lens(word_count_data=processor.labeled_train_sample_count_data,\n",
    "                                                                  strategy='max')\n",
    "scaled_unlabeled_data = processor.make_uniform_doc_lens(word_count_data=processor.unlabeled_count_data,\n",
    "                                                        strategy='max')\n",
    "scaled_test_data = processor.make_uniform_doc_lens(word_count_data=processor.test_count_data,\n",
    "                                                   strategy='max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 5250)\n",
      "[ 976.82142857  999.12328767 1012.00098619 1001.98913043  868.28571429\n",
      "  966.95454545  998.31884058  962.35        959.68421053 1006.0137931\n",
      "  993.51923077  982.3030303   962.35        979.23333333  984.05714286\n",
      "  970.79166667  810.4         975.48148148 1000.49382716  997.171875\n",
      " 1007.77835052  981.34375     985.62162162  990.48888889 1000.17721519\n",
      " 1000.64634146  980.32258065  972.48        983.20588235 1000.01282051\n",
      "  993.1372549   991.89583333  998.31884058  940.64285714  940.64285714\n",
      " 1006.86060606 1005.91608392  986.34210526 1001.74444444  959.68421053\n",
      " 1004.41525424 1010.39588689  995.83050847  928.58333333  996.66129032\n",
      " 1004.83064516  997.171875    844.16666667  959.68421053  940.64285714\n",
      " 1003.87387387  979.23333333  995.53448276  928.58333333  988.88095238\n",
      "  940.64285714  987.675       992.32653061  964.76190476  996.11666667\n",
      " 1001.61797753  868.28571429  935.07692308  994.91071429  953.41176471\n",
      "  992.32653061  940.64285714 1000.79518072  996.39344262 1001.61797753\n",
      "  886.375       997.41538462  996.11666667  996.11666667 1006.10884354\n",
      " 1008.12980769 1007.11046512  968.95652174 1005.76428571  974.03846154\n",
      "  976.82142857  993.1372549   966.95454545  976.82142857  975.48148148\n",
      "  997.88059701  984.86111111  968.95652174  990.48888889  993.1372549\n",
      " 1005.65942029  962.35        991.89583333  945.46666667  989.97727273\n",
      "  978.06896552  987.675       900.44444444 1005.02362205  968.95652174\n",
      " 1011.44631902  975.48148148  990.97826087  998.31884058 1007.49456522\n",
      "  994.58181818  991.89583333 1000.79518072 1006.46451613  990.97826087\n",
      " 1004.03539823  987.675       962.35        975.48148148  810.4\n",
      " 1008.17619048  983.20588235  979.23333333  964.76190476  990.48888889\n",
      "  997.41538462 1006.66875    1004.03539823  999.67105263 1008.6893617\n",
      "  981.34375    1003.53271028  968.95652174  900.44444444 1000.64634146\n",
      "  997.88059701  994.24074074  984.05714286 1010.86736842 1001.86813187\n",
      "  945.46666667 1000.79518072  974.03846154  989.97727273  998.73239437\n",
      "  956.72222222 1001.74444444  990.97826087 1001.98913043  953.41176471\n",
      " 1003.44339623  975.48148148  993.51923077  996.39344262  997.65151515\n",
      "  987.675      1007.30898876 1005.96527778  997.65151515  978.06896552\n",
      "  810.4         988.88095238 1002.97029703  999.84415584  996.66129032\n",
      " 1004.896       940.64285714 1008.4573991  1003.79090909  990.97826087\n",
      "  959.68421053  999.67105263  976.82142857  928.58333333  986.34210526\n",
      "  979.23333333  991.89583333  966.95454545 1007.83163265  996.39344262\n",
      "  982.3030303  1007.27683616  998.73239437  953.41176471  953.41176471\n",
      "  979.23333333  868.28571429  993.51923077 1001.08235294  999.12328767\n",
      "  976.82142857  968.95652174  964.76190476  979.23333333  972.48\n",
      "  979.23333333  959.68421053 1002.22340426 1008.7966805   995.22807018\n",
      "  986.34210526 1007.04117647  988.29268293]\n",
      "(500, 5250)\n",
      "[ 985.62162162  982.3030303   992.32653061    0.          984.86111111\n",
      "  928.58333333  978.06896552  868.28571429  987.675       985.62162162\n",
      " 1006.50641026  981.34375     990.48888889  949.6875     1011.49255952\n",
      "  987.02564103  940.64285714  886.375       959.68421053  953.41176471\n",
      "  506.5        1002.66326531  844.16666667  968.95652174  992.32653061\n",
      " 1007.960199    962.35        984.05714286  995.53448276  964.76190476\n",
      "  966.95454545  911.7         995.83050847  900.44444444  992.32653061\n",
      "  959.68421053  995.53448276  928.58333333  900.44444444  975.48148148\n",
      "  983.20588235  949.6875     1007.5828877   982.3030303   928.58333333\n",
      " 1004.11403509  900.44444444  999.31081081 1002.10752688 1000.17721519\n",
      " 1006.46451613  979.23333333  940.64285714 1000.3375      991.89583333\n",
      "  759.75       1000.64634146  966.95454545  975.48148148  953.41176471\n",
      "    0.          968.95652174  981.34375     998.73239437 1008.00985222\n",
      " 1000.3375      996.66129032  974.03846154  962.35        844.16666667\n",
      "  976.82142857  992.32653061  868.28571429  759.75        990.48888889\n",
      "  993.1372549   991.44680851  996.66129032  989.44186047  976.82142857\n",
      "  989.97727273  970.79166667 1009.36917563  911.7         949.6875\n",
      "  988.88095238  994.91071429  886.375       810.4         991.89583333\n",
      " 1006.33552632  984.05714286  956.72222222  953.41176471 1001.48863636\n",
      "  989.44186047  949.6875      964.76190476  868.28571429  988.29268293\n",
      "  959.68421053 1000.01282051  972.48        968.95652174  997.41538462\n",
      " 1012.61394817 1004.19130435  956.72222222  972.48        675.33333333\n",
      "  959.68421053  935.07692308  992.32653061  995.22807018  982.3030303\n",
      "  964.76190476  999.12328767  988.88095238  992.32653061 1003.06862745\n",
      "  962.35        868.28571429    0.            0.          911.7\n",
      "    0.          959.68421053  959.68421053 1000.49382716  995.22807018\n",
      "  975.48148148  935.07692308  900.44444444  997.171875    968.95652174\n",
      "  940.64285714  956.72222222  995.53448276  920.90909091  974.03846154\n",
      "  979.23333333 1011.20390071 1006.89759036  506.5         974.03846154\n",
      "  980.32258065  976.82142857  920.90909091  987.02564103 1001.86813187\n",
      "  994.24074074  989.97727273  935.07692308  844.16666667 1006.42207792\n",
      "  970.79166667  972.48        991.44680851  989.44186047  993.51923077\n",
      "  978.06896552  900.44444444 1004.11403509  956.72222222 1011.17477477\n",
      "  920.90909091  974.03846154 1011.64209115  972.48        991.44680851\n",
      "  985.62162162  844.16666667  984.05714286  983.20588235  911.7\n",
      "  992.74        998.93055556  984.86111111  987.675       949.6875\n",
      " 1008.81404959  940.64285714  979.23333333  953.41176471  953.41176471\n",
      "  935.07692308  976.82142857  911.7         980.32258065  935.07692308\n",
      "  900.44444444  994.58181818  886.375       972.48        953.41176471\n",
      "  953.41176471  992.74       1005.0859375   990.48888889  964.76190476\n",
      "  998.73239437  935.07692308    0.          928.58333333  993.1372549\n",
      "  959.68421053  993.1372549   940.64285714 1008.7966805   966.95454545\n",
      "  990.48888889  986.34210526  968.95652174  987.02564103  959.68421053\n",
      "  984.05714286  998.93055556  984.86111111  997.41538462 1007.27683616\n",
      "    0.          506.5         886.375       997.41538462 1008.00985222\n",
      "  949.6875        0.          844.16666667  987.675       920.90909091\n",
      "  920.90909091  979.23333333 1006.86060606  999.84415584  956.72222222\n",
      "  976.82142857  989.44186047  998.93055556 1001.35632184  935.07692308\n",
      "  996.66129032 1003.95535714  959.68421053  900.44444444  949.6875\n",
      "    0.          994.24074074  945.46666667  994.58181818  928.58333333\n",
      "  759.75        956.72222222  928.58333333  979.23333333  940.64285714\n",
      "  975.48148148 1012.60857805  996.39344262  981.34375     997.65151515\n",
      " 1004.34188034  956.72222222 1002.55670103  983.20588235  675.33333333\n",
      "  992.32653061  945.46666667 1007.75129534  999.31081081    0.\n",
      "  962.35        992.74        953.41176471 1009.30291971  886.375\n",
      "  959.68421053 1000.17721519  978.06896552 1006.33552632  810.4\n",
      "  994.58181818  953.41176471  978.06896552 1002.33684211  935.07692308\n",
      "  987.02564103  810.4         868.28571429    0.          900.44444444\n",
      "  982.3030303   970.79166667  506.5           0.          964.76190476\n",
      " 1009.56610169  999.67105263  970.79166667 1003.95535714  976.82142857\n",
      "  945.46666667 1001.61797753 1004.62809917  928.58333333 1007.07602339\n",
      "  979.23333333  966.95454545  993.1372549   949.6875      976.82142857\n",
      "  997.171875    962.35        995.22807018  886.375       984.86111111\n",
      "  987.675       886.375       949.6875      962.35        970.79166667\n",
      "  940.64285714  981.34375     962.35        953.41176471  759.75\n",
      "  978.06896552  953.41176471  968.95652174  844.16666667  920.90909091\n",
      "  975.48148148  970.79166667  984.86111111  675.33333333  759.75\n",
      "  959.68421053  985.62162162  982.3030303   844.16666667  998.73239437\n",
      "    0.          988.88095238  949.6875      993.1372549   968.95652174\n",
      "  675.33333333 1000.79518072  940.64285714  968.95652174  844.16666667\n",
      "  974.03846154 1005.4962963   920.90909091  975.48148148    0.\n",
      "  984.05714286  506.5         940.64285714  989.97727273  975.48148148\n",
      "  987.675       997.65151515    0.          962.35        986.34210526\n",
      "  999.12328767 1003.79090909  991.89583333    0.          959.68421053\n",
      "  928.58333333 1009.10384615  962.35        886.375      1002.76767677\n",
      "  759.75        675.33333333  979.23333333  949.6875      987.675\n",
      "  997.88059701 1009.40780142  962.35        940.64285714  935.07692308\n",
      " 1000.49382716  935.07692308  956.72222222  968.95652174 1000.3375\n",
      " 1002.22340426  911.7         675.33333333  972.48        970.79166667\n",
      "  920.90909091  993.88679245  980.32258065  993.88679245  962.35\n",
      "  984.86111111  985.62162162  920.90909091 1007.960199   1008.03431373\n",
      "  993.51923077  956.72222222  949.6875      964.76190476  979.23333333\n",
      "  966.95454545  962.35        911.7         964.76190476    0.\n",
      "  989.44186047  974.03846154  998.73239437 1002.44791667  984.05714286\n",
      "  949.6875      998.10294118  956.72222222  928.58333333  868.28571429\n",
      " 1008.61471861  986.34210526  995.22807018  975.48148148  940.64285714\n",
      " 1002.44791667  989.44186047  990.97826087  985.62162162  991.89583333\n",
      "  911.7         810.4         868.28571429  935.07692308 1001.35632184\n",
      " 1006.06164384  982.3030303   997.41538462  928.58333333  994.58181818\n",
      "  959.68421053  940.64285714 1003.95535714  984.86111111 1003.53271028\n",
      " 1007.14450867  974.03846154  989.44186047  920.90909091  990.97826087\n",
      " 1008.39545455  940.64285714 1005.96527778  935.07692308  970.79166667\n",
      "  985.62162162 1002.66326531  984.86111111  976.82142857  976.82142857\n",
      "  945.46666667  675.33333333  988.88095238    0.          995.83050847\n",
      " 1008.53744493 1007.24431818  935.07692308  983.20588235  996.66129032\n",
      "  966.95454545  940.64285714  968.95652174  940.64285714 1009.08880309\n",
      "  953.41176471 1007.90954774  998.73239437  968.95652174  675.33333333\n",
      "  986.34210526  997.171875    968.95652174  810.4         997.171875\n",
      " 1000.3375      968.95652174  959.68421053  997.171875    964.76190476\n",
      "  980.32258065  900.44444444  945.46666667 1004.41525424  984.86111111]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_labeled_train_sample_data.shape)\n",
    "print(np.sum(scaled_labeled_train_sample_data, axis=processor.vocab_axis))\n",
    "print(scaled_unlabeled_data.shape)\n",
    "print(np.sum(scaled_unlabeled_data, axis=processor.vocab_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample has 20 unique labels\n",
      "Checking initail M step on only labeled train data...\n",
      "Congrats, initial M step assertions passed.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = EM_SSL(labeled_count_data=scaled_labeled_train_sample_data,\n",
    "               label_vals=processor.train_sample_label_vals,\n",
    "               unlabeled_count_data=scaled_unlabeled_data,\n",
    "               max_em_iters=2,\n",
    "               min_em_loss_delta=2e-4)\n",
    "\n",
    "#model.fit()\n",
    "model.initialize_EM()  # only runs M step (compute thetas) on labeled train samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classs in labeled train 20\n",
      "1.8632873729477415\n"
     ]
    }
   ],
   "source": [
    "print('num classs in labeled train', len(model.label_set))\n",
    "print(model.word_counts_per_class[0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(model.total_word_count_per_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently only labeled data: False\n",
      "true label of single doc: 18\n",
      "Denominator terms\n",
      "\n",
      "total word count for class  18 8938.085593139323\n",
      "min expected proba: 7.048167234651812e-05\n",
      "log of min expected proba -9.56015784865931\n",
      "Numerator terms\n",
      "\n",
      "labeled max word counts for class 18: 180.89285714285717\n",
      "single doc word count min, avg: 0.0 0.19276209260824648\n",
      "theta_j_vocab min for class 18: 7.048167234651812e-05\n"
     ]
    }
   ],
   "source": [
    "# problem to solve, when computing: labeled loss log (P(yi = cj|θ) .P(xi|yi = cj; θ)) ,\n",
    "# P(xi|yi = cj; θ) -> 0 => log() -> nan\n",
    "# and the reason P(xi|yi = cj; θ) -> 0 is bc theta_j_vocab -> 0 => theta_j_vocab^w_t -> 0 for w_t > 0\n",
    "# Recall  P(xi|yi = cj; θ) = product_t \\theta_j_t^ w_t\n",
    "## So log(P(xi|yi = cj; θ) ) = sum_t (w_t * log(\\theta_j_t())\n",
    "### So just need theta_j_t > 0, likelihood improved with smaller vocab size or smaller unif. doc length\n",
    "\n",
    "# TODO: implement above log trick for theta_j_vocab\n",
    "\n",
    "print('currently only labeled data:', model.only_labeled_data)\n",
    "single_doc = model.labeled_count_data[2]\n",
    "this_true_label =  model.label_vals[2]\n",
    "print('true label of single doc:', this_true_label)\n",
    "# Denominators for single doc per class proba\n",
    "print('Denominator terms\\n')\n",
    "print('total word count for class % d' % this_true_label, model.total_word_count_per_class[this_true_label])\n",
    "# compute minimum proba: for words not appearing in class\n",
    "min_expected_proba = (1 / (model.vocab_size + model.total_word_count_per_class[this_true_label]))\n",
    "print('min expected proba:', min_expected_proba)\n",
    "print('log of min expected proba', np.log(min_expected_proba))\n",
    "print('Numerator terms\\n')\n",
    "# Numerators for single doc per class proba\n",
    "print('labeled max word counts for class %d:' % this_true_label, model.labeled_word_counts_per_class[this_true_label].max())\n",
    "print('single doc word count min, avg:', single_doc.min(), single_doc.mean())\n",
    "print('theta_j_vocab min for class %d:' % this_true_label, model.theta_j_vocab_per_class[this_true_label].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS ON SINGLE TRAIN LABEL DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check P(c_j | theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta j shape (20,)\n",
      "sum theta j == 1:  True\n"
     ]
    }
   ],
   "source": [
    "theta_j = model.theta_j_per_class  # per class: (n_docs_in_class + 1) / (self.n_docs + self.n_labels)\n",
    "print('theta j shape', theta_j.shape)\n",
    "print('sum theta j == 1: ', np.isclose(np.sum(theta_j, axis=0), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FIX: P(x_i | c_j, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of log_prob(w_t | cj): (20, 5250)\n",
      "log_prob(w_t | cj) min, mean, max\n",
      "-9.85741110514188 -9.365035524016424 -4.0178290863837915\n",
      "doc unnorm LOG probas per class of single do avg, min: -9282.708699945166 -9636.60654707081\n",
      "TEST doc unnorm LOG probas per class of single do avg, min: -9282.708699945166 -9636.60654707081\n",
      "argmax unnorm LOG proba class 18\n",
      "TEST argmax unnorm LOG proba class 18\n"
     ]
    }
   ],
   "source": [
    "# FIX: P(x_i | c_j, theta) -> {theta}_{tj} =: theta_j_vocab -> non-nan computations\n",
    "X_log = np.log(model.theta_j_vocab_per_class)\n",
    "print('shape of log_prob(w_t | cj):', X_log.shape)\n",
    "print('log_prob(w_t | cj) min, mean, max')\n",
    "print(X_log.min(), X_log.mean(), X_log.max())\n",
    "\n",
    "# COMPUTE UNNORMALIZED LOG PROBAS\n",
    "doc_log_proba_per_class = np.log(model.theta_j_per_class) +  np.array([np.sum(single_doc * np.log(model.theta_j_vocab_per_class[j]), axis=0)\n",
    "                                            for j in model.ordered_labels_list])\n",
    "test_doc_log_proba_per_class = model.compute_unnormalized_class_log_probas_doc(single_doc)\n",
    "print('doc unnorm LOG probas per class of single do avg, min:', doc_log_proba_per_class.mean(), doc_log_proba_per_class.min())\n",
    "print('TEST doc unnorm LOG probas per class of single do avg, min:', test_doc_log_proba_per_class.mean(), test_doc_log_proba_per_class.min())\n",
    "print('argmax unnorm LOG proba class', np.argmax(doc_log_proba_per_class))\n",
    "print('TEST argmax unnorm LOG proba class', np.argmax(test_doc_log_proba_per_class))\n",
    "\n",
    "# fixed_doc_proba_per_class = np.exp(doc_log_proba_per_class)\n",
    "# print('FIXED doc probas per class of single do avg, min:', fixed_doc_proba_per_class.mean(), fixed_doc_proba_per_class.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test doc log probas per class (20,)\n",
      "given unnormalized log probas prior to logSumExp [-9292.468323   -9471.71868692 -9329.26109874 -9333.25223776\n",
      " -9636.60654707 -9449.38210979 -9404.00320765 -9521.44397521\n",
      " -9527.51914768 -9437.64775702 -9189.48547816 -9220.05069891\n",
      " -9360.95182688 -9249.30744261 -9331.70213934 -9448.99069339\n",
      " -9153.420478   -9055.80120098 -7921.77217105 -9319.38877874]\n",
      "summand [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "log of sums -7921.7721710469705\n",
      "test log probas [-1370.69615195 -1549.94651587 -1407.48892769 -1411.48006671\n",
      " -1714.83437602 -1527.60993875 -1482.23103661 -1599.67180417\n",
      " -1605.74697664 -1515.87558597 -1267.71330711 -1298.27852786\n",
      " -1439.17965583 -1327.53527156 -1409.9299683  -1527.21852234\n",
      " -1231.64830695 -1134.02902994     0.         -1397.6166077 ]\n",
      "dtype test log probas float64\n",
      "test probas normalized [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "test probas norm idx 3 0.0\n",
      "argmax:  18\n"
     ]
    }
   ],
   "source": [
    "print('shape of test doc log probas per class', test_doc_log_proba_per_class.shape)\n",
    "#print('log sum', test_doc_log_proba_per_class.sum())\n",
    "#def compute_sum_of_logs_from_log_prbas(log_probas):\n",
    "#\"\"\"For single doc x, compute denom: log P(x|theta), without yet using P(c_j|theta)\"\"\"\n",
    "log_probas = np.copy(test_doc_log_proba_per_class)\n",
    "print('given unnormalized log probas prior to logSumExp', log_probas)\n",
    "# log sum exp trick: see Murphy\n",
    "max_log = np.max(log_probas)\n",
    "summand = [np.exp(a - max_log) for a in log_probas]  # still have underflow...\n",
    "print('summand', summand)\n",
    "log_of_sums = np.log(np.sum(summand)) + max_log  #+ min_expected_proba # log(denom)\n",
    "print('log of sums', log_of_sums)\n",
    "test_log_probas_normalized  = log_probas - log_of_sums\n",
    "print('test log probas', test_log_probas_normalized)\n",
    "print('dtype test log probas', test_log_probas_normalized.dtype)\n",
    "test_probas_normalized = np.exp(test_log_probas_normalized)\n",
    "print('test probas normalized', test_probas_normalized)\n",
    "print('test probas norm idx 3', test_probas_normalized[3])\n",
    "print('argmax: ', np.argmax(test_probas_normalized))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X = np.sum(np.exp(log_probas - max_log))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_log_probas [-9292.468323   -9471.71868692 -9329.26109874 -9333.25223776\n",
      " -9636.60654707 -9449.38210979 -9404.00320765 -9521.44397521\n",
      " -9527.51914768 -9437.64775702 -9189.48547816 -9220.05069891\n",
      " -9360.95182688 -9249.30744261 -9331.70213934 -9448.99069339\n",
      " -9153.420478   -9055.80120098 -7921.77217105 -9319.38877874]\n"
     ]
    }
   ],
   "source": [
    "# formalize as methods\n",
    "\n",
    "def compute_unnormalized_class_log_probas(doc):\n",
    "    \"\"\"Get unnormalized log probas\"\"\"\n",
    "    u_log_probas =  np.log(model.theta_j_per_class) +  np.array([np.sum(doc * np.log(model.theta_j_vocab_per_class[j]), axis=0)\n",
    "                                            for j in model.ordered_labels_list])\n",
    "    return u_log_probas\n",
    "\n",
    "\n",
    "    \n",
    "def compute_doc_class_probas(u_log_probas):\n",
    "    \"\"\"Apply normalization compute normalization factor using log-sum-exp trick.\"\"\"\n",
    "    max_log = np.max(u_log_probas)\n",
    "    summand = np.sum(np.exp(log_probas - max_log))\n",
    "    log_of_sums = np.log(summand) + max_log  #+ min_expected_proba # log(denom)\n",
    "    log_probas_normalized  = log_probas - log_of_sums\n",
    "    class_probas_normalized = np.exp(log_probas_normalized)\n",
    "    return class_probas_normalized\n",
    "\n",
    "u_log_probas = compute_unnormalized_class_log_probas(single_doc)\n",
    "print('u_log_probas', u_log_probas)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE CLOSER LOOK AT FIRST THETAS ON LABELED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 5250)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labeled_count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.unlabeled_this_class_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "12.0\n",
      "7.0\n",
      "8.0\n",
      "12.0\n",
      "9.0\n",
      "9.0\n",
      "11.0\n",
      "13.0\n",
      "13.0\n",
      "8.0\n",
      "10.0\n",
      "13.0\n",
      "8.0\n",
      "9.0\n",
      "14.0\n",
      "6.0\n",
      "11.0\n",
      "9.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.ordered_labels_list:\n",
    "    print(model.n_labeled_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.class_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "12.0\n",
      "7.0\n",
      "8.0\n",
      "12.0\n",
      "9.0\n",
      "9.0\n",
      "11.0\n",
      "13.0\n",
      "13.0\n",
      "8.0\n",
      "10.0\n",
      "13.0\n",
      "8.0\n",
      "9.0\n",
      "14.0\n",
      "6.0\n",
      "11.0\n",
      "9.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.label_set:\n",
    "    print(model.n_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00019047619047619048\n",
      "(20, 5250)\n",
      "word count avg, theta_j, theta_jt_avg, theta_jt_max\n",
      "1.86, 0.05, 0.00, 0.01\n",
      "2.24, 0.06, 0.00, 0.01\n",
      "1.33, 0.04, 0.00, 0.01\n",
      "1.49, 0.04, 0.00, 0.01\n",
      "2.24, 0.06, 0.00, 0.02\n",
      "1.68, 0.05, 0.00, 0.01\n",
      "1.67, 0.05, 0.00, 0.01\n",
      "2.00, 0.06, 0.00, 0.01\n",
      "2.38, 0.06, 0.00, 0.01\n",
      "2.40, 0.06, 0.00, 0.01\n",
      "1.49, 0.04, 0.00, 0.02\n",
      "1.88, 0.05, 0.00, 0.01\n",
      "2.44, 0.06, 0.00, 0.01\n",
      "1.52, 0.04, 0.00, 0.01\n",
      "1.68, 0.05, 0.00, 0.01\n",
      "2.64, 0.07, 0.00, 0.01\n",
      "1.12, 0.03, 0.00, 0.01\n",
      "2.07, 0.06, 0.00, 0.01\n",
      "1.70, 0.05, 0.00, 0.01\n",
      "1.10, 0.03, 0.00, 0.02\n"
     ]
    }
   ],
   "source": [
    "print(1 / model.vocab_size)\n",
    "print(model.theta_j_vocab_per_class.shape)\n",
    "\n",
    "print(\"word count avg, theta_j, theta_jt_avg, theta_jt_max\")\n",
    "for j in model.label_set:\n",
    "    print(\"%0.2f, %0.2f, %0.2f, %0.2f\" % \n",
    "          (model.labeled_word_counts_per_class[j].mean(), model.theta_j_per_class[j], \n",
    "           model.theta_j_vocab_per_class[j].mean(), model.theta_j_vocab_per_class[j].max()))\n",
    "# TODO: theta_jt near zero => log(theta_jt) -> nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03211009174311927"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_per_class[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.1038429994154453 0.0 197.72459893048128 5795.175746931088\n"
     ]
    }
   ],
   "source": [
    "print(model.word_counts_per_class[j].min(), model.word_counts_per_class[j].mean(), \n",
    "      model.word_counts_per_class[j].min(), model.word_counts_per_class[j].max(), \n",
    "    model.word_counts_per_class[j].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5795.175746931088"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.total_word_count_per_class[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_vocab_per_class[j].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001904761904761905, 0.017991981611129828)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_vocab_per_class[j].mean(), model.theta_j_vocab_per_class[j].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't want to compute np.log(model.theta_j_vocab_per_class) directly since theta_j_vocab is sparse => underflow\n",
    "#np.log(model.theta_j_vocab_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Train fit: sanity check\n",
    "pct_train_correct_preds = model.evaluate_on_data(count_data=scaled_labeled_train_sample_data,\n",
    "                                            label_vals=processor.train_sample_label_vals)\n",
    "print(pct_train_correct_preds)  # without a full EM loop, should be equal to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2506\n",
      "0.33271375464684017\n"
     ]
    }
   ],
   "source": [
    "# out-of-sammple inference: test\n",
    "pct_test_correct_preds = model.evaluate_on_data(count_data=scaled_test_data,\n",
    "                                            label_vals=processor.full_test_label_vals)\n",
    "print(pct_test_correct_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(model.preds.dtype)\n",
    "print(processor.full_test_label_vals.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.33271375464684017\n"
     ]
    }
   ],
   "source": [
    "m = model.preds\n",
    "p =  processor.full_test_label_vals\n",
    "print(m.shape == p.shape)\n",
    "corr = m == p\n",
    "print(np.sum(corr) / len(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
