{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.corpus import words as nltk_english_words\n",
    "\n",
    "from lib_utils.preprocessing import TextPreProcessor\n",
    "from lib_utils.expectation_maximization import EM_SSL\n",
    "\n",
    "\n",
    "\n",
    "# Set tokens to remove for all text preprocessing\n",
    "remove_zero_vocab_docs = True\n",
    "english_vocab = set(nltk_english_words.words())\n",
    "english_vocab = None\n",
    "_tokens_to_remove = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train raw data shape: (11314,)\n",
      "min, max full train data label vals = 0, 19\n",
      "unlabeled train data shape: (1000,)\n",
      "num avail train indices complement to unlabeled: 10314\n"
     ]
    }
   ],
   "source": [
    "# Fix static preprocessed data\n",
    "# original article suggests 10k fixed unlabeled samples\n",
    "processor = TextPreProcessor(n_labeled_train_samples=100,\n",
    "    n_unlabeled_train_samples=500,\n",
    "                                    tokens_to_remove=_tokens_to_remove,\n",
    "                                    remove_zero_vocab_docs=remove_zero_vocab_docs,\n",
    "                                    english_vocab=english_vocab)\n",
    "# Initialize raw\n",
    "processor.set_static_full_train_raw_data()\n",
    "processor.set_static_raw_unlabeled_data()\n",
    "processor.set_static_raw_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train samples to select: 100\n",
      "at set_sample_raw_train_data\n",
      "size: (100,)\n",
      "sample labeled trained sentence\n",
      " \n",
      "Another dodge.  Oh well.  I'm no match for your amazing repertoire\n",
      "of red herrings and smoke screens.  \n",
      "\n",
      "You asked for an apology.  I'm not going to apologize for pointing out\n",
      "that your straw-man argument was a straw-man argument.  Nor for saying\n",
      "that your list of \"bible contradictions\" shows such low standards of\n",
      "scholarship that it should be an embarrassment to anti-inerrantists,\n",
      "just as Josh McDowell should be an embarrassment to the fundies.  Nor\n",
      "for objecting various times to your taking quotes out of context.  Nor\n",
      "for pointing out that \"they do it too\" is not an excuse. Nor for calling\n",
      "your red herrings and smoke screens what they are.\n",
      "\n",
      "I'm still not sure why you think I'm a hypocrite.  It's true that I\n",
      "haven't responded to any of Robert Weiss' articles, which may be due in\n",
      "part to the fact that I almost never read his articles.  But I have\n",
      "responded to both you and Frank DeCenso (a fundie/inerrantist.)  Both\n",
      "you and Frank have taken quotes out of context, and I've objected to\n",
      "both of you doing so.  I've criticized bad arguments both when they\n",
      "were yours and I agreed with the conclusion (that the Bible is not\n",
      "inerrant), and when they were Frank's and I disagreed with the\n",
      "conclusion.  I've criticized both you and Frank for evading questions,\n",
      "and for trying to \"explain me away\" without addressing the objections\n",
      "I raise (you by accusing me of being hypocritical and irrational, Frank\n",
      "by accusing me of being motivated by a desire to attack the Bible.) I\n",
      "don't see that any of this is hypocritical, nor do I apologize for it.\n",
      "\n",
      "I do apologize, however, for having offended you in any other way.\n",
      "\n",
      "Happy now?\n",
      "Warning in process_documents_text: received single doc as str, not array; converting to array\n",
      "processed sentence\n",
      " ['another dodge oh well match amazing repertoire red herrings smoke screens asked apology going apologize pointing straw man argument straw man argument saying list bible contradictions shows low standards scholarship embarrassment anti inerrantists josh mcdowell embarrassment fundies objecting various times taking quotes context pointing excuse calling red herrings smoke screens still sure think hypocrite true responded robert weiss articles may due part fact almost never read articles responded frank decenso fundie inerrantist frank taken quotes context objected criticized bad arguments agreed conclusion bible inerrant frank disagreed conclusion criticized frank evading questions trying explain away without addressing objections raise accusing hypocritical irrational frank accusing motivated desire attack bible see hypocritical apologize apologize however offended way happy']\n"
     ]
    }
   ],
   "source": [
    "processor.set_sample_raw_train_data()\n",
    "sample_sent = processor.labeled_train_data_sample[0]\n",
    "print('sample labeled trained sentence\\n', sample_sent)\n",
    "processed_sent = processor.process_documents_text(sample_sent)\n",
    "print('processed sentence\\n', processed_sent)\n",
    "\n",
    "doc_array_sample = processor.labeled_train_data_sample[:2]\n",
    "processed_docs = processor.process_documents_text(doc_array_sample)  # List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample count data before zero doc removal shape (100, 5400)\n",
      "Removing zero vocab docs from labeled train sample.\n",
      "After zero count doc removal:\n",
      " Kept 95 samples from original 11314 train\n",
      "labeled_train_sample_count_data shape: (95, 5400)\n",
      "got data=unlabeled, shape= (1000,)\n",
      "unlabeled count data shape (1000, 5400)\n",
      "unlabeld train count_data shape: (1000, 5400)\n",
      "test count data shape (7532, 5400)\n"
     ]
    }
   ],
   "source": [
    "# doc-to-vect based on train sample's count vectorizer\n",
    "processor.set_labeled_train_sample_count_data()\n",
    "print('labeled_train_sample_count_data shape:', processor.labeled_train_sample_count_data.shape)\n",
    "processor.set_unlabeled_count_data()\n",
    "print('unlabeld train count_data shape:', processor.unlabeled_count_data.shape)\n",
    "processor.set_test_count_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400\n",
      "['aau', 'abc', 'abcdefghijklmnopqrstuvwxyz', 'ability', 'able', 'abortion', 'absolute', 'absolutism', 'abuse', 'abusers']\n",
      "['yourhost', 'yourname', 'ysu', 'za', 'zabriskie', 'zeus', 'zia', 'zimbabve', 'zogwarg', 'zone']\n",
      "['ac', 'ad', 'ah', 'ai', 'al', 'au', 'aw', 'ax', 'bc', 'ca', 'cc', 'cd', 'cf', 'ch', 'cm', 'co', 'cs', 'cu', 'cv', 'cz', 'db', 'dc', 'de', 'dg', 'di', 'dk', 'dr', 'ds', 'du', 'dx', 'ec', 'ed', 'ee', 'eg', 'eq', 'er', 'et', 'fd', 'fi', 'fr', 'ft', 'gc', 'ge', 'gm', 'go', 'hh', 'hi', 'hl', 'hp', 'hz', 'id', 'im', 'io', 'jz', 'la', 'ld', 'le', 'lp', 'mh', 'mr', 'ms', 'nd', 'ne', 'nl', 'ns', 'nt', 'ob', 'oh', 'oj', 'ok', 'os', 'pa', 'pm', 'ps', 'rh', 'rs', 'sa', 'sf', 'sh', 'sq', 'st', 'sw', 'sy', 'th', 'tm', 'tn', 'tr', 'tv', 'tx', 'ty', 'uh', 'ui', 'uk', 'un', 'us', 'uu', 'va', 'vm', 'vs', 'wc', 'wd', 'wu', 'xa', 'xc', 'xl', 'xt', 'xv', 'xz', 'ya', 'za']\n"
     ]
    }
   ],
   "source": [
    "# CHECK TRAIN SET VOCAB\n",
    "print(len(processor.vocab))\n",
    "print(processor.vocab[:10])\n",
    "print(processor.vocab[-10:])\n",
    "short_vocab_text = [v for v in processor.vocab if len(v) <=2]\n",
    "print(short_vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.0\n",
      "4550\n"
     ]
    }
   ],
   "source": [
    "# LABELED TRAIN COUNT DATA STATS\n",
    "processor.get_train_doc_lengths()\n",
    "print(processor.med_doc_len)\n",
    "print(processor.max_doc_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scale count data to trains' unif doc len\n",
    "scaled_labeled_train_sample_data = processor.make_uniform_doc_lens(word_count_data=processor.labeled_train_sample_count_data,\n",
    "                                                                  strategy='max')\n",
    "scaled_unlabeled_data = processor.make_uniform_doc_lens(word_count_data=processor.unlabeled_count_data,\n",
    "                                                        strategy='max')\n",
    "scaled_test_data = processor.make_uniform_doc_lens(word_count_data=processor.test_count_data,\n",
    "                                                   strategy='max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 5400)\n",
      "[4510.77586207 4282.35294118 4265.625      4514.453125   4412.12121212\n",
      " 4381.48148148 4387.5        4510.0877193  4510.43478261 4427.02702703\n",
      " 4095.         4333.33333333 4265.625      4265.625      4310.52631579\n",
      " 4439.02439024 4387.5        4548.83780332 4265.625      3791.66666667\n",
      " 3640.         4225.         4310.52631579 4459.         4548.70775348\n",
      " 4472.88135593 4455.20833333 4527.5862069  4527.47524752 4044.44444444\n",
      " 4481.06060606 4044.44444444 4542.26190476 4529.86725664 3981.25\n",
      " 4468.75       4536.33633634 4530.63829787 4439.02439024 4412.12121212\n",
      " 4478.90625    4282.35294118 4477.77777778 4496.47058824 4451.08695652\n",
      " 4474.16666667 4368.         4246.66666667 4200.         4503.09278351\n",
      " 4352.17391304 3981.25       4494.51219512 4549.00021973 4490.13157895\n",
      " 4477.77777778 4475.40983607 4398.33333333 4487.67123288 4464.1509434\n",
      " 4482.08955224 4457.14285714 4444.18604651 4333.33333333 4483.08823529\n",
      " 4368.         4412.12121212 4465.74074074 4441.66666667 4453.19148936\n",
      " 4297.22222222 4423.61111111 4297.22222222 4515.78947368 4403.22580645\n",
      " 4502.60416667 4439.02439024 4492.40506329 4412.12121212 4368.\n",
      " 3412.5        4510.0877193  4490.13157895 4533.33333333 4535.41666667\n",
      " 4536.92528736 4507.0754717  4375.         4460.78431373 4333.33333333\n",
      " 4543.16816817 4246.66666667 3033.33333333 4393.10344828 4444.18604651]\n",
      "(1000, 5400)\n",
      "[4485.         4484.05797101 4526.17801047 3640.         4136.36363636\n",
      " 4333.33333333 4095.         4460.78431373 4387.5        4534.7826087\n",
      " 4352.17391304 4540.40084388 4423.61111111 4265.625      4490.13157895\n",
      " 4044.44444444 4310.52631579 4441.66666667 4398.33333333 4095.\n",
      " 4504.95049505 4453.19148936 4512.08333333 4407.8125     4441.66666667\n",
      " 4485.         4462.5        4495.18072289 4360.41666667 4444.18604651\n",
      " 4360.41666667 4475.40983607 4448.88888889 4200.         4476.61290323\n",
      "    0.         4282.35294118 4343.18181818 4427.02702703    0.\n",
      " 4200.         4333.33333333 4282.35294118 4433.33333333 4459.\n",
      " 4507.87037037 4511.76470588 3900.         4525.6684492  4504.95049505\n",
      " 4310.52631579 4381.48148148 4465.74074074 4541.19922631 4136.36363636\n",
      " 4460.78431373 4507.0754717     0.         4393.10344828 3412.5\n",
      " 4403.22580645 4246.66666667 4368.         4246.66666667 4433.33333333\n",
      " 4333.33333333 4352.17391304 4136.36363636 4381.48148148 4265.625\n",
      " 4521.3836478  3640.         4436.25       4333.33333333 4448.88888889\n",
      " 4352.17391304 4420.         4471.55172414 4265.625      4333.33333333\n",
      "    0.         3791.66666667 4282.35294118 4343.18181818    0.\n",
      " 4486.80555556 4368.         4407.8125     3981.25       4398.33333333\n",
      " 4246.66666667 4044.44444444 4407.8125     4044.44444444 4360.41666667\n",
      " 2275.         4310.52631579 4322.5        4265.625      4427.02702703\n",
      " 4375.         4387.5        4451.08695652    0.         4333.33333333\n",
      " 4282.35294118 4548.66842259 4427.02702703 4516.2962963  4170.83333333\n",
      " 4322.5        4095.         4423.61111111 4448.88888889 4504.5\n",
      " 4537.32590529 4520.83333333 4246.66666667 4430.26315789 4352.17391304\n",
      " 4225.         3900.         3900.         4200.         4322.5\n",
      " 4265.625      4507.0754717  4497.70114943 4200.         4530.472103\n",
      " 4343.18181818 4498.29545455 4322.5        4225.         4343.18181818\n",
      " 3900.         4490.90909091    0.         4387.5        4044.44444444\n",
      "    0.         4246.66666667 4507.87037037 4265.625      4444.18604651\n",
      " 4398.33333333 3981.25       4407.8125     4387.5        4433.33333333\n",
      "    0.            0.         4416.17647059 4333.33333333 4546.10445205\n",
      " 3900.         4448.88888889 4427.02702703 4310.52631579 4481.06060606\n",
      " 4375.         3791.66666667 3791.66666667 4519.46308725 4343.18181818\n",
      " 4477.77777778 4368.         4044.44444444 4427.02702703 4225.\n",
      " 4509.73451327 4333.33333333 4499.44444444 3412.5        4453.19148936\n",
      " 4423.61111111 4467.27272727 4503.57142857 4095.         4487.67123288\n",
      " 4095.         4200.         4478.90625    4453.19148936 4430.26315789\n",
      " 4412.12121212 4503.09278351 4352.17391304 4170.83333333 4474.16666667\n",
      " 4360.41666667 4170.83333333 4200.         4430.26315789 4446.59090909\n",
      " 4478.90625    4436.25       4225.         4503.57142857 4457.14285714\n",
      " 4170.83333333 4297.22222222    0.         4343.18181818 4451.08695652\n",
      " 4412.12121212 4430.26315789    0.         4136.36363636 4468.75\n",
      " 4488.51351351 4465.74074074 4467.27272727 4497.09302326 4387.5\n",
      " 4510.0877193  4044.44444444 4246.66666667 4322.5        4282.35294118\n",
      " 4360.41666667 4170.83333333 4430.26315789 4476.61290323 4427.02702703\n",
      " 4297.22222222 4200.         4403.22580645 4246.66666667 4246.66666667\n",
      " 4433.33333333 3981.25       4508.25688073 4453.19148936 4485.\n",
      " 4375.         4360.41666667 3900.         4541.91829485 3900.\n",
      " 4403.22580645 4464.1509434  4459.         3900.         3981.25\n",
      " 3981.25       3981.25       4513.30645161 4407.8125     4510.0877193\n",
      " 4200.         4433.33333333 4505.39215686 4246.66666667 3033.33333333\n",
      " 4485.         4448.88888889 4453.19148936 4322.5        4282.35294118\n",
      " 4322.5           0.         4505.39215686 4375.         4416.17647059\n",
      " 4533.86524823 4368.         4322.5        4246.66666667 4381.48148148\n",
      " 4225.            0.         4297.22222222 4520.45454545 4333.33333333\n",
      " 4333.33333333 4427.02702703 4491.66666667 3033.33333333 4423.61111111\n",
      " 4297.22222222 4481.06060606 4482.08955224 4495.83333333 4095.\n",
      " 4535.86956522 4530.472103   4420.         4546.73835125 4499.44444444\n",
      " 4526.30208333 4511.11111111 4453.19148936 4352.17391304 3791.66666667\n",
      " 4387.5        4495.83333333 4465.74074074 4407.8125     4509.375\n",
      " 4528.43601896 4487.67123288 4322.5        4542.50411862 4044.44444444\n",
      " 4265.625      4246.66666667 4246.66666667 4423.61111111 4170.83333333\n",
      " 4200.         4360.41666667 4433.33333333 4322.5        4495.83333333\n",
      " 4439.02439024 4333.33333333 4095.         4403.22580645 4333.33333333\n",
      " 4527.36318408 4265.625      4403.22580645 4310.52631579 3791.66666667\n",
      " 4518.18181818 4455.20833333 4416.17647059 4436.25       4136.36363636\n",
      " 3033.33333333 4297.22222222 4439.02439024 3900.         4387.5\n",
      " 4416.17647059 4459.         4483.08823529    0.         4455.20833333\n",
      " 4532.36434109 4333.33333333 4493.82716049 4381.48148148 4381.48148148\n",
      " 4282.35294118 4467.27272727 4310.52631579 4508.63636364 3900.\n",
      " 4520.06578947 4136.36363636 4459.         4282.35294118 4482.08955224\n",
      " 4460.78431373 4482.08955224 4095.         3791.66666667 4423.61111111\n",
      " 4528.63849765 4433.33333333 3791.66666667 4136.36363636 4502.10526316\n",
      " 4375.         4509.375      4531.94444444 4548.45500849 4472.88135593\n",
      " 4483.08823529 4457.14285714 4044.44444444 4448.88888889 4225.\n",
      " 4548.58122856 4282.35294118 4506.25       3981.25       4423.61111111\n",
      " 4490.13157895 4453.19148936 4387.5        4516.04477612 4500.54347826\n",
      " 3412.5        4427.02702703 4465.74074074 3900.         4360.41666667\n",
      " 4265.625      3900.         4471.55172414 4510.0877193  4436.25\n",
      " 4436.25       4460.78431373 4322.5        4520.06578947 4322.5\n",
      " 4420.         4375.         4360.41666667 4375.            0.\n",
      " 4170.83333333 4368.         4398.33333333 4430.26315789 4310.52631579\n",
      " 4485.91549296 4462.5        4352.17391304 4472.88135593 4322.5\n",
      " 4368.         4483.08823529 3640.            0.         4493.125\n",
      " 3981.25       4519.86754967 4444.18604651 4310.52631579 4095.\n",
      " 4407.8125     4493.125      4412.12121212 4436.25       4475.40983607\n",
      " 4496.47058824 4497.70114943 4493.125      3900.         4343.18181818\n",
      " 4462.5        4282.35294118 4333.33333333 4044.44444444 4297.22222222\n",
      " 4507.0754717  4526.54639175 4246.66666667 4310.52631579 4322.5\n",
      " 4439.02439024 4200.         4095.         4504.95049505 4200.\n",
      " 4044.44444444 4246.66666667 4375.         4297.22222222 4484.05797101\n",
      " 4470.1754386     0.         4478.90625    4489.33333333 4548.62827856\n",
      " 4436.25       4511.11111111 4517.02898551 4352.17391304 4416.17647059\n",
      " 4282.35294118 4136.36363636    0.         3033.33333333 4508.25688073\n",
      " 4439.02439024 4474.16666667 3640.         4200.         4225.\n",
      " 4436.25       4467.27272727 4472.88135593 4503.09278351 4136.36363636\n",
      " 4375.         4044.44444444 4360.41666667 3791.66666667 4333.33333333\n",
      " 4322.5        3791.66666667 4095.         4444.18604651 4136.36363636\n",
      " 4472.88135593 4439.02439024 4265.625      4200.         4470.1754386\n",
      " 4225.         3640.         4459.         4095.         4503.09278351\n",
      " 4549.46808511 4297.22222222 4375.         4200.         4508.25688073\n",
      " 4508.25688073 4393.10344828 4514.72868217 4310.52631579 4455.20833333\n",
      " 4398.33333333 4495.18072289 4530.72033898 4333.33333333 4529.41176471\n",
      " 4136.36363636 4352.17391304 4471.55172414 4462.5        4095.\n",
      " 4200.         4044.44444444 4333.33333333 4136.36363636 4352.17391304\n",
      " 4471.55172414    0.         4501.07526882 4489.33333333 4477.77777778\n",
      " 4136.36363636 3900.         4539.3442623  4246.66666667    0.\n",
      " 4489.33333333 4297.22222222 4510.43478261 3640.         4360.41666667\n",
      " 4420.         4375.         4333.33333333 4136.36363636 4476.61290323\n",
      " 3981.25       4412.12121212 3791.66666667 4333.33333333 4524.72222222\n",
      " 4509.73451327 4170.83333333 4387.5        4433.33333333 4044.44444444\n",
      " 4246.66666667    0.         4360.41666667 4510.77586207 4398.33333333\n",
      "    0.         4482.08955224 4457.14285714 4381.48148148 4459.\n",
      " 3033.33333333 4095.         4448.88888889 4521.91358025 4515.26717557\n",
      " 4225.         4368.         4420.         4464.1509434  3791.66666667\n",
      " 4297.22222222 4282.35294118 4095.         4403.22580645 4387.5\n",
      " 4509.73451327 4360.41666667 4474.16666667 4381.48148148 4485.91549296\n",
      " 4508.63636364 4136.36363636 4381.48148148 4430.26315789 4536.77325581\n",
      "    0.         3900.         4484.05797101 4393.10344828 4412.12121212\n",
      " 4475.40983607 4095.         4333.33333333 4526.78571429 4539.3442623\n",
      " 4491.66666667    0.         4310.52631579 4459.         4381.48148148\n",
      " 3412.5        4420.         4282.35294118 4477.77777778 4407.8125\n",
      " 4420.         3981.25       4497.09302326 4095.         4044.44444444\n",
      " 4310.52631579 4480.         4521.3836478  4170.83333333 4451.08695652\n",
      " 4322.5        4534.03508772 4512.39669421 3412.5        4297.22222222\n",
      " 4407.8125     4483.08823529 4375.         4200.         4265.625\n",
      " 4352.17391304 3033.33333333 4485.         4477.77777778 4521.3836478\n",
      " 3791.66666667 4436.25       4393.10344828 4136.36363636 4393.10344828\n",
      " 4368.         4499.44444444 4297.22222222 4471.55172414 4483.08823529\n",
      " 4398.33333333 4381.48148148 4381.48148148 4246.66666667 4483.08823529\n",
      " 4412.12121212 4282.35294118 4532.15686275 4444.18604651 4485.91549296\n",
      "    0.         4044.44444444 4403.22580645 4265.625      4459.\n",
      " 4095.         4504.5        4297.22222222 4343.18181818 4462.5\n",
      " 4497.70114943    0.         4333.33333333 4297.22222222 4484.05797101\n",
      " 4282.35294118 3900.         4509.73451327 4352.17391304 4547.41917187\n",
      " 4136.36363636 4322.5        4200.         4462.5        4170.83333333\n",
      " 4343.18181818 4375.         4485.         4343.18181818 4476.61290323\n",
      " 4310.52631579 4507.47663551 4444.18604651 4506.25       4225.\n",
      " 4477.77777778 4457.14285714 4375.         4527.36318408 4310.52631579\n",
      " 4265.625      4360.41666667 4498.29545455 4282.35294118 4044.44444444\n",
      " 4381.48148148 4495.83333333 4549.56442657 4200.         4477.77777778\n",
      " 4444.18604651 4407.8125     4510.77586207 4265.625      4448.88888889\n",
      " 4136.36363636 4427.02702703 4200.         4453.19148936 4489.33333333\n",
      " 3791.66666667 4310.52631579 4536.96275072    0.         4483.08823529\n",
      " 4136.36363636 4095.         4455.20833333 4465.74074074 4515.26717557\n",
      " 3640.         4333.33333333 4333.33333333 4453.19148936 4531.12033195\n",
      " 4407.8125     4478.90625    4485.         4485.         4439.02439024\n",
      " 4433.33333333 4360.41666667 4540.54054054 4534.09090909 4333.33333333\n",
      " 4542.08695652 4246.66666667 4170.83333333 4282.35294118 3640.\n",
      " 4368.         4509.73451327 3981.25       4246.66666667 4225.\n",
      " 4513.6        4457.14285714 4457.14285714 4044.44444444 4412.12121212\n",
      " 4444.18604651 4523.23529412 4416.17647059 4407.8125     4375.\n",
      " 4170.83333333 4044.44444444 4387.5        3900.         4496.47058824\n",
      " 4200.         4475.40983607 4095.         4407.8125     3412.5\n",
      " 4416.17647059 4441.66666667 4497.09302326 4453.19148936 4246.66666667\n",
      " 4536.21212121 4487.67123288 4322.5        4520.83333333 4468.75\n",
      " 4545.34764826 4136.36363636 4407.8125     3412.5        3900.\n",
      " 4136.36363636 2275.         4482.08955224 4477.77777778 4501.59574468\n",
      " 4368.         4453.19148936 4310.52631579 4462.5        4495.18072289\n",
      " 4532.08661417 4407.8125        0.         4542.87949922 4520.83333333\n",
      " 4375.         4407.8125     4507.87037037 4439.02439024 4444.18604651\n",
      " 4398.33333333 4444.18604651 4546.67640614 4523.07692308 4170.83333333\n",
      "    0.         4170.83333333 4136.36363636 4352.17391304 4095.\n",
      " 4297.22222222 4509.375      4170.83333333 4381.48148148 4375.\n",
      " 4477.77777778 4381.48148148 4511.76470588 4515.26717557    0.\n",
      " 4343.18181818 4398.33333333 4505.39215686 4505.82524272 4322.5\n",
      " 4433.33333333    0.         4322.5        4517.02898551 4282.35294118\n",
      " 4381.48148148 4503.09278351 4403.22580645 4423.61111111 4548.80608764\n",
      " 4246.66666667 3791.66666667 4044.44444444 4136.36363636 4322.5\n",
      " 4501.59574468    0.         4501.07526882 4516.04477612 4472.88135593\n",
      " 4464.1509434  4343.18181818 4225.         4393.10344828 4170.83333333\n",
      " 4538.05774278 4333.33333333 4368.         3900.         3412.5\n",
      " 4246.66666667 4225.         4467.27272727 4398.33333333 4310.52631579\n",
      " 3791.66666667 4407.8125     4246.66666667 4375.         4095.\n",
      " 4455.20833333 4170.83333333 4475.40983607 4398.33333333 4333.33333333\n",
      " 4381.48148148 4441.66666667 4427.02702703 4477.77777778 4343.18181818\n",
      " 4490.90909091 4282.35294118 4246.66666667 4333.33333333 4477.77777778\n",
      " 4471.55172414    0.         4368.         4448.88888889 4483.08823529\n",
      " 4387.5        3981.25       4472.88135593 4423.61111111 4446.59090909\n",
      " 4387.5        4453.19148936 4430.26315789 4282.35294118 4282.35294118\n",
      " 4444.18604651 4489.33333333 4393.10344828 4477.77777778 4381.48148148\n",
      " 4352.17391304 4525.13661202 4200.         4509.375      4265.625\n",
      " 4472.88135593 4136.36363636 4322.5           0.         4282.35294118\n",
      " 4322.5        4381.48148148 4398.33333333 3640.         4486.80555556\n",
      " 3981.25       4265.625      4480.         4439.02439024 4095.\n",
      " 4459.         4495.83333333 4441.66666667 3412.5        4200.\n",
      " 4297.22222222 4333.33333333 4200.         4436.25       4430.26315789\n",
      " 4464.1509434  4534.57627119 4483.08823529 3981.25       2275.\n",
      " 4095.         4360.41666667 4246.66666667 4282.35294118 4375.\n",
      " 4398.33333333 4439.02439024 4476.61290323 4451.08695652 4393.10344828\n",
      " 4322.5        4507.87037037 4387.5        4492.40506329 4548.8372093\n",
      " 4500.         4387.5        4486.80555556 2275.         3900.\n",
      " 4352.17391304 4523.07692308 3033.33333333 3412.5        4412.12121212\n",
      "    0.         4368.            0.         4282.35294118 4282.35294118\n",
      " 4548.80389064 3640.         4407.8125        0.         4488.51351351\n",
      " 4476.61290323 4282.35294118 4462.5        4505.39215686 4518.40277778\n",
      " 4352.17391304 4427.02702703 4368.         3640.            0.\n",
      " 4490.13157895 4200.         3791.66666667 4170.83333333 4465.74074074\n",
      " 4297.22222222 4462.5        4420.         4136.36363636    0.\n",
      " 4412.12121212    0.         4095.         4459.         4515.53030303]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_labeled_train_sample_data.shape)\n",
    "print(np.sum(scaled_labeled_train_sample_data, axis=processor.vocab_axis))\n",
    "print(scaled_unlabeled_data.shape)\n",
    "print(np.sum(scaled_unlabeled_data, axis=processor.vocab_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled train sample has 20 unique labels\n",
      "Checking initail M step on only labeled train data...\n",
      "Congrats, initial M step assertions passed.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = EM_SSL(labeled_count_data=scaled_labeled_train_sample_data,\n",
    "               label_vals=processor.train_sample_label_vals,\n",
    "               unlabeled_count_data=scaled_unlabeled_data,\n",
    "               max_em_iters=2,\n",
    "               min_em_loss_delta=2e-4)\n",
    "\n",
    "#model.fit()\n",
    "model.initialize_EM()  # only runs M step (compute thetas) on labeled train samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n",
      "5.6870573871153685\n"
     ]
    }
   ],
   "source": [
    "print(model.label_set)\n",
    "print(model.word_counts_per_class[0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(model.total_word_count_per_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently only labeled data: False\n",
      "true label of single doc: 2\n",
      "Denominator terms\n",
      "\n",
      "total word count for class  2 29460.3029520076\n",
      "min expected proba: 2.8685923968495235e-05\n",
      "log of min expected proba -10.45910400957003\n",
      "Numerator terms\n",
      "\n",
      "labeled max word counts for class 2: 1516.6666666666667\n",
      "single doc word count min, avg: 0.0 0.7899305555555556\n",
      "theta_j_vocab min for class 2: 2.8685923968495235e-05\n"
     ]
    }
   ],
   "source": [
    "# problem to solve, when computing: labeled loss log (P(yi = cj|θ) .P(xi|yi = cj; θ)) ,\n",
    "# P(xi|yi = cj; θ) -> 0 => log() -> nan\n",
    "# and the reason P(xi|yi = cj; θ) -> 0 is bc theta_j_vocab -> 0 => theta_j_vocab^w_t -> 0 for w_t > 0\n",
    "# Recall  P(xi|yi = cj; θ) = product_t \\theta_j_t^ w_t\n",
    "## So log(P(xi|yi = cj; θ) ) = sum_t (w_t * log(\\theta_j_t())\n",
    "### So just need theta_j_t > 0, likelihood improved with smaller vocab size or smaller unif. doc length\n",
    "\n",
    "# TODO: implement above log trick for theta_j_vocab\n",
    "\n",
    "print('currently only labeled data:', model.only_labeled_data)\n",
    "single_doc = model.labeled_count_data[2]\n",
    "this_true_label =  model.label_vals[2]\n",
    "print('true label of single doc:', this_true_label)\n",
    "# Denominators for single doc per class proba\n",
    "print('Denominator terms\\n')\n",
    "print('total word count for class % d' % this_true_label, model.total_word_count_per_class[this_true_label])\n",
    "# compute minimum proba: for words not appearing in class\n",
    "min_expected_proba = (1 / (model.vocab_size + model.total_word_count_per_class[this_true_label]))\n",
    "print('min expected proba:', min_expected_proba)\n",
    "print('log of min expected proba', np.log(min_expected_proba))\n",
    "print('Numerator terms\\n')\n",
    "# Numerators for single doc per class proba\n",
    "print('labeled max word counts for class %d:' % this_true_label, model.labeled_word_counts_per_class[this_true_label].max())\n",
    "print('single doc word count min, avg:', single_doc.min(), single_doc.mean())\n",
    "print('theta_j_vocab min for class %d:' % this_true_label, model.theta_j_vocab_per_class[this_true_label].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS ON SINGLE TRAIN LABEL DOCUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check P(c_j | theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta j shape (20,)\n",
      "sum theta j == 1:  True\n"
     ]
    }
   ],
   "source": [
    "theta_j = model.theta_j_per_class  # per class: (n_docs_in_class + 1) / (self.n_docs + self.n_labels)\n",
    "print('theta j shape', theta_j.shape)\n",
    "print('sum theta j == 1: ', np.isclose(np.sum(theta_j, axis=0), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FIX: P(x_i | c_j, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of log_prob(w_t | cj): (20, 5400)\n",
      "log_prob(w_t | cj) min, mean, max\n",
      "-10.71499899360082 -9.893558274134527 -3.1341746629033578\n",
      "doc LOG probas per class of single do avg, min: -39054.46717747797 -41867.52672056465\n",
      "TEST doc LOG probas per class of single do avg, min: -39054.46717747797 -41867.52672056465\n",
      "argmax LOG proba class 2\n",
      "TEST argmax LOG proba class 2\n"
     ]
    }
   ],
   "source": [
    "# FIX: P(x_i | c_j, theta) -> {theta}_{tj} =: theta_j_vocab -> non-nan computations\n",
    "X_log = np.log(model.theta_j_vocab_per_class)\n",
    "print('shape of log_prob(w_t | cj):', X_log.shape)\n",
    "print('log_prob(w_t | cj) min, mean, max')\n",
    "print(X_log.min(), X_log.mean(), X_log.max())\n",
    "doc_log_proba_per_class = np.array([np.sum(single_doc * np.log(model.theta_j_vocab_per_class[j]), axis=0)\n",
    "                                            for j in model.ordered_labels_list])\n",
    "test_doc_log_proba_per_class = model.compute_doc_log_proba_per_class(single_doc)\n",
    "print('doc LOG probas per class of single do avg, min:', doc_log_proba_per_class.mean(), doc_log_proba_per_class.min())\n",
    "print('TEST doc LOG probas per class of single do avg, min:', test_doc_log_proba_per_class.mean(), test_doc_log_proba_per_class.min())\n",
    "print('argmax LOG proba class', np.argmax(doc_log_proba_per_class))\n",
    "print('TEST argmax LOG proba class', np.argmax(test_doc_log_proba_per_class))\n",
    "\n",
    "# fixed_doc_proba_per_class = np.exp(doc_log_proba_per_class)\n",
    "# print('FIXED doc probas per class of single do avg, min:', fixed_doc_proba_per_class.mean(), fixed_doc_proba_per_class.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test doc log probas per class (20,)\n",
      "log of sums -19644.13403510143\n",
      "test log probas [-22223.39268546 -21698.87314649      0.         -18432.42137385\n",
      " -21043.11890256 -17700.11815543 -21195.29590106 -22123.27663283\n",
      " -21300.09491257 -22100.01083794 -18503.69576939 -20573.46892123\n",
      " -19641.990833   -20060.16343001 -22192.23808851 -18671.49836286\n",
      " -21513.0614009  -19055.94256372 -19592.68489489 -20585.31603481]\n",
      "dtype test log probas float64\n",
      "test probas normalized [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "test probas norm idx 3 0.0\n",
      "argmax:  2\n"
     ]
    }
   ],
   "source": [
    "print('shape of test doc log probas per class', test_doc_log_proba_per_class.shape)\n",
    "#print('log sum', test_doc_log_proba_per_class.sum())\n",
    "#def compute_sum_of_logs_from_log_prbas(log_probas):\n",
    "#\"\"\"For single doc x, compute denom: log P(x|theta), without yet using P(c_j|theta)\"\"\"\n",
    "log_probas = np.copy(test_doc_log_proba_per_class)\n",
    "# log sum exp trick: see Murphy\n",
    "max_log = np.max(log_probas)\n",
    "summand = np.sum([np.exp(a - max_log) for a in log_probas])\n",
    "log_of_sums = np.log(summand) + max_log  #+ min_expected_proba # log(denom)\n",
    "print('log of sums', log_of_sums)\n",
    "test_log_probas_normalized  = log_probas - log_of_sums\n",
    "print('test log probas', test_log_probas_normalized)\n",
    "print('dtype test log probas', test_log_probas_normalized.dtype)\n",
    "test_probas_normalized = np.exp(test_log_probas_normalized)\n",
    "print('test probas normalized', test_probas_normalized)\n",
    "print('test probas norm idx 3', test_probas_normalized[3])\n",
    "print('argmax: ', np.argmax(test_probas_normalized))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKE CLOSER LOOK AT FIRST THETAS ON LABELED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 5400)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labeled_count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.unlabeled_this_class_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "5.0\n",
      "3.0\n",
      "3.0\n",
      "5.0\n",
      "6.0\n",
      "3.0\n",
      "2.0\n",
      "6.0\n",
      "2.0\n",
      "4.0\n",
      "3.0\n",
      "9.0\n",
      "7.0\n",
      "4.0\n",
      "1.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.ordered_labels_list:\n",
    "    print(model.n_labeled_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.class_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "5.0\n",
      "3.0\n",
      "3.0\n",
      "5.0\n",
      "6.0\n",
      "3.0\n",
      "2.0\n",
      "6.0\n",
      "2.0\n",
      "4.0\n",
      "3.0\n",
      "9.0\n",
      "7.0\n",
      "4.0\n",
      "1.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "for j in model.label_set:\n",
    "    print(model.n_docs_per_class[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018518518518518518\n",
      "(20, 5400)\n",
      "word count avg, theta_j, theta_jt_avg, theta_jt_max\n",
      "5.69, 0.07, 0.00, 0.02\n",
      "5.44, 0.07, 0.00, 0.03\n",
      "5.46, 0.07, 0.00, 0.04\n",
      "5.76, 0.07, 0.00, 0.01\n",
      "4.11, 0.05, 0.00, 0.03\n",
      "2.50, 0.03, 0.00, 0.02\n",
      "2.49, 0.03, 0.00, 0.03\n",
      "4.07, 0.05, 0.00, 0.03\n",
      "4.87, 0.06, 0.00, 0.03\n",
      "2.29, 0.03, 0.00, 0.04\n",
      "1.65, 0.03, 0.00, 0.03\n",
      "4.94, 0.06, 0.00, 0.02\n",
      "1.65, 0.03, 0.00, 0.03\n",
      "3.29, 0.04, 0.00, 0.02\n",
      "2.37, 0.03, 0.00, 0.04\n",
      "7.34, 0.09, 0.00, 0.03\n",
      "5.63, 0.07, 0.00, 0.02\n",
      "3.35, 0.04, 0.00, 0.01\n",
      "0.83, 0.02, 0.00, 0.04\n",
      "3.15, 0.04, 0.00, 0.04\n"
     ]
    }
   ],
   "source": [
    "print(1 / model.vocab_size)\n",
    "print(model.theta_j_vocab_per_class.shape)\n",
    "\n",
    "print(\"word count avg, theta_j, theta_jt_avg, theta_jt_max\")\n",
    "for j in model.label_set:\n",
    "    print(\"%0.2f, %0.2f, %0.2f, %0.2f\" % \n",
    "          (model.labeled_word_counts_per_class[j].mean(), model.theta_j_per_class[j], \n",
    "           model.theta_j_vocab_per_class[j].mean(), model.theta_j_vocab_per_class[j].max()))\n",
    "# TODO: theta_jt near zero => log(theta_jt) -> nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043478260869565216"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_per_class[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.154225376249928 0.0 949.2241379310345 17032.81703174961\n"
     ]
    }
   ],
   "source": [
    "print(model.word_counts_per_class[j].min(), model.word_counts_per_class[j].mean(), \n",
    "      model.word_counts_per_class[j].min(), model.word_counts_per_class[j].max(), \n",
    "    model.word_counts_per_class[j].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17032.81703174961"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.total_word_count_per_class[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_vocab_per_class[j].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00018518518518518518, 0.042358663050929515)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta_j_vocab_per_class[j].mean(), model.theta_j_vocab_per_class[j].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't want to compute np.log(model.theta_j_vocab_per_class) directly since theta_j_vocab is sparse => underflow\n",
    "#np.log(model.theta_j_vocab_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EM_SSL' object has no attribute 'compute_doc_proba_per_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-3598a640e7a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# out-of-sammple inference: test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m pct_test_correct_preds = model.evaluate_on_data(count_data=scaled_test_data,\n\u001b[1;32m----> 3\u001b[1;33m                                             label_vals=processor.full_test_label_vals)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpct_test_correct_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\SSL\\SemiSupervisedLearning\\lib_utils\\expectation_maximization.py\u001b[0m in \u001b[0;36mevaluate_on_data\u001b[1;34m(self, count_data, label_vals)\u001b[0m\n\u001b[0;32m    389\u001b[0m         pred_probas = np.apply_along_axis(func1d=self.compute_normalized_class_probas_doc,\n\u001b[0;32m    390\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_axis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                                           arr=count_data)\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         ) from None\n\u001b[1;32m--> 379\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\SSL\\SemiSupervisedLearning\\lib_utils\\expectation_maximization.py\u001b[0m in \u001b[0;36mcompute_normalized_class_probas_doc\u001b[1;34m(self, doc_word_counts)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mUsed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcompute\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0mof\u001b[0m \u001b[0munlabeled\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mEM\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0munnormalized_class_probas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_unnormalized_class_probas_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_word_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc_word_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munnormalized_class_probas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# if np.isclose(denom, 0):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\SSL\\SemiSupervisedLearning\\lib_utils\\expectation_maximization.py\u001b[0m in \u001b[0;36mcompute_unnormalized_class_probas_doc\u001b[1;34m(self, doc_word_counts)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \"\"\"\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# TODO: CONVERT TO LOG SPACE: ADD THETAS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mdoc_probas_per_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_doc_proba_per_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_word_counts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc_word_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[1;31m# u_class_probas_doc = np.prod([self.theta_j_per_class, doc_probas_per_class], axis=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mu_class_probas_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta_j_per_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_probas_per_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EM_SSL' object has no attribute 'compute_doc_proba_per_class'"
     ]
    }
   ],
   "source": [
    "# out-of-sammple inference: test\n",
    "pct_test_correct_preds = model.evaluate_on_data(count_data=scaled_test_data,\n",
    "                                            label_vals=processor.full_test_label_vals)\n",
    "print(pct_test_correct_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.preds.dtype)\n",
    "print(processor.full_test_label_vals.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.preds\n",
    "p =  processor.full_test_label_vals\n",
    "print(m.shape == p.shape)\n",
    "corr = m == p\n",
    "print(np.sum(corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
